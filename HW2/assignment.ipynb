{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval 1#\n",
    "## Assignment 2: Retrieval models [100 points] ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will get familiar with basic and advanced information retrieval concepts. You will implement different information retrieval ranking models and evaluate their performance.\n",
    "\n",
    "We provide you with a Indri index. To query the index, you'll use a Python package ([pyndri](https://github.com/cvangysel/pyndri)) that allows easy access to the underlying document statistics.\n",
    "\n",
    "For evaluation you'll use the [TREC Eval](https://github.com/usnistgov/trec_eval) utility, provided by the National Institute of Standards and Technology of the United States. TREC Eval is the de facto standard way to compute Information Retrieval measures and is frequently referenced in scientific papers.\n",
    "\n",
    "This is a **groups-of-three assignment**, the deadline is **Wednesday, January 31st**. Code quality, informative comments and convincing analysis of the results will be considered when grading. Submission should be done through blackboard, questions can be asked on the course [Piazza](piazza.com/university_of_amsterdam/spring2018/52041inr6y/home).\n",
    "\n",
    "### Technicalities (must-read!) ###\n",
    "\n",
    "The assignment directory is organized as follows:\n",
    "   * `./assignment.ipynb` (this file): the description of the assignment.\n",
    "   * `./index/`: the index we prepared for you.\n",
    "   * `./ap_88_90/`: directory with ground-truth and evaluation sets:\n",
    "      * `qrel_test`: test query relevance collection (**test set**).\n",
    "      * `qrel_validation`: validation query relevance collection (**validation set**).\n",
    "      * `topics_title`: semicolon-separated file with query identifiers and terms.\n",
    "\n",
    "You will need the following software packages (tested with Python 3.5 inside [Anaconda](https://conda.io/docs/user-guide/install/index.html)):\n",
    "   * Python 3.5 and Jupyter\n",
    "   * Indri + Pyndri (Follow the installation instructions [here](https://github.com/nickvosk/pyndri/blob/master/README.md))\n",
    "   * gensim [link](https://radimrehurek.com/gensim/install.html)\n",
    "   * TREC Eval [link](https://github.com/usnistgov/trec_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREC Eval primer ###\n",
    "The TREC Eval utility can be downloaded and compiled as follows:\n",
    "\n",
    "    git clone https://github.com/usnistgov/trec_eval.git\n",
    "    cd trec_eval\n",
    "    make\n",
    "\n",
    "TREC Eval computes evaluation scores given two files: ground-truth information regarding relevant documents, named *query relevance* or *qrel*, and a ranking of documents for a set of queries, referred to as a *run*. The *qrel* will be supplied by us and should not be changed. For every retrieval model (or combinations thereof) you will generate a run of the top-1000 documents for every query. The format of the *run* file is as follows:\n",
    "\n",
    "    $query_identifier Q0 $document_identifier $rank_of_document_for_query $query_document_similarity $run_identifier\n",
    "    \n",
    "where\n",
    "   * `$query_identifier` is the unique identifier corresponding to a query (usually this follows a sequential numbering).\n",
    "   * `Q0` is a legacy field that you can ignore.\n",
    "   * `$document_identifier` corresponds to the unique identifier of a document (e.g., APXXXXXXX where AP denotes the collection and the Xs correspond to a unique numerical identifier).\n",
    "   * `$rank_of_document_for_query` denotes the rank of the document for the particular query. This field is ignored by TREC Eval and is only maintained for legacy support. The ranks are computed by TREC Eval itself using the `$query_document_similarity` field (see next). However, it remains good practice to correctly compute this field.\n",
    "   * `$query_document_similarity` is a score indicating the similarity between query and document where a higher score denotes greater similarity.\n",
    "   * `$run_identifier` is an identifier of the run. This field is for your own convenience and has no purpose beyond bookkeeping.\n",
    "   \n",
    "For example, say we have two queries: `Q1` and `Q2` and we rank three documents (`DOC1`, `DOC2`, `DOC3`). For query `Q1`, we find the following similarity scores `score(Q1, DOC1) = 1.0`, `score(Q1, DOC2) = 0.5`, `score(Q1, DOC3) = 0.75`; and for `Q2`: `score(Q2, DOC1) = -0.1`, `score(Q2, DOC2) = 1.25`, `score(Q1, DOC3) = 0.0`. We can generate run using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 Q0 DOC1 1 11.0 example\n",
      "Q1 Q0 DOC3 2 0.75 example\n",
      "Q1 Q0 DOC2 3 0.5 example\n",
      "Q2 Q0 DOC2 1 1.25 example\n",
      "Q2 Q0 DOC3 2 0.0 example\n",
      "Q2 Q0 DOC1 3 -0.1 example\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def write_run(model_name, data, out_f,\n",
    "              max_objects_per_query=sys.maxsize,\n",
    "              skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            logging.warning('Received empty ranking for %s; ignoring.',\n",
    "                            subject_id)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "        \n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))\n",
    "            \n",
    "# The following writes the run to standard output.\n",
    "# In your code, you should write the runs to local\n",
    "# storage in order to pass them to trec_eval.\n",
    "write_run(\n",
    "    model_name='example',\n",
    "    data={\n",
    "        'Q1': ((11.0, 'DOC1'), (0.5, 'DOC2'), (0.75, 'DOC3')),\n",
    "        'Q2': ((-0.1, 'DOC1'), (1.25, 'DOC2'), (0.0, 'DOC3')),\n",
    "    },\n",
    "    out_f=sys.stdout,\n",
    "    max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#actually writing to example.run\n",
    "fd = open('example.run','w+')\n",
    "write_run(\n",
    "    model_name='example',\n",
    "    data={\n",
    "        'Q1': ((1.0, 'DOC1'), (0.5, 'DOC2'), (0.75, 'DOC3')),\n",
    "        'Q2': ((-0.1, 'DOC1'), (1.25, 'DOC2'), (0.0, 'DOC3')),\n",
    "    },\n",
    "    out_f=fd,\n",
    "    max_objects_per_query=1000)\n",
    "fd.close()\n",
    "\n",
    "#actually writing to example.qrel\n",
    "fd = open('example.qrel', 'w+')\n",
    "fd.write(\"Q1 0 DOC1 1 \\nQ1 0 DOC3 0 \\nQ2 0 DOC3 1\")\n",
    "fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run file:\n",
      "Q2 Q0 DOC2 1 1.25 example\n",
      "Q2 Q0 DOC3 2 0.0 example\n",
      "Q2 Q0 DOC1 3 -0.1 example\n",
      "Q1 Q0 DOC1 1 1.0 example\n",
      "Q1 Q0 DOC3 2 0.75 example\n",
      "Q1 Q0 DOC2 3 0.5 example\n",
      "\n",
      "query relevance file:\n",
      "Q1 0 DOC1 1 \n",
      "Q1 0 DOC3 0 \n",
      "Q2 0 DOC3 1"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#testing file content\n",
    "echo 'run file:'\n",
    "cat example.run\n",
    "echo ''\n",
    "echo 'query relevance file:'\n",
    "cat example.qrel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that we know that `DOC1` is relevant and `DOC3` is non-relevant for `Q1`. In addition, for `Q2` we only know of the relevance of `DOC3`. The query relevance file looks like:\n",
    "\n",
    "    Q1 0 DOC1 1\n",
    "    Q1 0 DOC3 0\n",
    "    Q2 0 DOC3 1\n",
    "    \n",
    "We store the run and qrel in files `example.run` and `example.qrel` respectively on disk. We can now use TREC Eval to compute evaluation measures. In this example, we're only interested in Mean Average Precision and we'll only show this below for brevity. However, TREC Eval outputs much more information such as NDCG, recall, precision, etc.\n",
    "\n",
    "    $ trec_eval -m all_trec -q example.qrel example.run | grep -E \"^map\\s\"\n",
    "    > map                   \tQ1\t1.0000\n",
    "    > map                   \tQ2\t0.5000\n",
    "    > map                   \tall\t0.7500\n",
    "    \n",
    "Now that we've discussed the output format of rankings and how you can compute evaluation measures from these rankings, we'll now proceed with an overview of the indexing framework you'll use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_5                   \tQ1\t0.2000\n",
      "recall_1000           \tQ1\t1.0000\n",
      "ndcg_cut_10           \tQ1\t1.0000\n",
      "map_cut_1000          \tQ1\t1.0000\n",
      "P_5                   \tQ2\t0.2000\n",
      "recall_1000           \tQ2\t1.0000\n",
      "ndcg_cut_10           \tQ2\t0.6309\n",
      "map_cut_1000          \tQ2\t0.5000\n",
      "P_5                   \tall\t0.2000\n",
      "recall_1000           \tall\t1.0000\n",
      "ndcg_cut_10           \tall\t0.8155\n",
      "map_cut_1000          \tall\t0.7500\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./trec_eval/trec_eval -m all_trec -q example.qrel example.run | grep -E \"^ndcg_cut_10\\s|^map_cut_1000\\s|^P_5\\s|^recall_1000\\s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyndri primer ###\n",
    "For this assignment you will use [Pyndri](https://github.com/cvangysel/pyndri) [[1](https://arxiv.org/abs/1701.00749)], a python interface for [Indri](https://www.lemurproject.org/indri.php). We have indexed the document collection and you can query the index using Pyndri. We will start by giving you some examples of what Pyndri can do:\n",
    "\n",
    "First we read the document collection index with Pyndri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyndri\n",
    "\n",
    "index = pyndri.Index('index/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If it crashes!!!\n",
    "Nichita ( Based on what people said on Piazza ):\n",
    "\n",
    "You can try this:\n",
    "\n",
    "1. go to index/31/manifest\n",
    "2. there's a line \"<indri-distribution>Indri development release 5.8</indri-distribution>\"\n",
    "\n",
    "3. change \"Indri development release 5.8\" to \"Indri release 5.11\"\n",
    "\n",
    "Run previous cell again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded index can be used to access a collection of documents in an easy manner. We'll give you some examples to get some idea of what it can do, it is up to you to figure out how to use it for the remainder of the assignment.\n",
    "\n",
    "First let's look at the number of documents, since Pyndri indexes the documents using incremental identifiers we can simply take the lowest index and the maximum document and consider the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 164597 documents in this collection.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are %d documents in this collection.\" % (index.maximum_document() - index.document_base()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first document out of the collection and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AP890425-0001', (1360, 192, 363, 0, 880, 0, 200, 0, 894, 412, 92160, 3, 192, 0, 363, 34, 1441, 0, 174134, 0, 200, 0, 894, 412, 2652, 0, 810, 107, 49, 4903, 420, 0, 1, 48, 35, 489, 0, 35, 687, 192, 243, 0, 249311, 1877, 0, 1651, 1174, 0, 2701, 117, 412, 0, 810, 391, 245233, 1225, 5838, 16, 0, 233156, 3496, 0, 393, 17, 0, 2435, 4819, 930, 0, 0, 200, 0, 894, 0, 22, 398, 145, 0, 3, 271, 115, 0, 1176, 2777, 292, 0, 725, 192, 0, 0, 50046, 0, 1901, 1130, 0, 192, 0, 408, 0, 243779, 0, 0, 553, 192, 0, 363, 0, 3747, 0, 0, 0, 0, 1176, 0, 1239, 0, 0, 1115, 17, 0, 0, 585, 192, 1963, 0, 0, 412, 54356, 0, 773, 0, 0, 0, 192, 0, 0, 1130, 0, 363, 0, 545, 192, 0, 1174, 1901, 1130, 0, 4, 398, 145, 39, 0, 577, 0, 355, 0, 491, 0, 6025, 0, 0, 193156, 88, 34, 437, 0, 0, 1852, 0, 828, 0, 1588, 0, 0, 0, 2615, 0, 0, 107, 49, 420, 0, 0, 190, 7, 714, 2701, 0, 237, 192, 157, 0, 412, 34, 437, 0, 0, 200, 6025, 26, 0, 0, 0, 0, 363, 0, 22, 398, 145, 0, 200, 638, 126222, 6018, 0, 880, 0, 0, 161, 0, 0, 319, 894, 2701, 0, 0, 0, 301, 1200, 0, 363, 251, 430, 0, 207, 0, 76143, 1773, 0, 243779, 0, 0, 72030, 0, 55, 4903, 420, 0, 2701, 1496, 420, 0, 25480, 0, 420, 0, 0, 200, 0, 392, 2949, 0, 1738, 0, 61, 0, 71, 79, 0, 200, 903, 0, 188, 53, 6, 0, 476, 2, 0, 2028, 97, 334, 0, 0, 200, 178, 0, 0, 107, 49, 0, 214, 0, 0, 0, 114, 3866, 1505, 195, 79893, 574, 0, 198, 2160, 0, 192, 0, 420, 0, 384, 0, 2701, 0, 114, 6025, 1549, 74627, 0, 238, 0, 0, 0, 3729, 0, 192, 0, 79893, 0, 0, 729, 3141, 129, 0, 192, 196764, 39, 0, 0, 714, 63, 0, 55, 420, 3356, 0, 0, 117, 412, 0, 0, 79758, 0, 1901, 1130, 4067, 2133, 0, 0, 875, 72, 0, 0, 336, 2789, 0, 0, 25, 920, 121, 104, 0, 3162, 0, 0, 420, 0, 2178, 0, 0, 386, 192545, 159306, 0, 0, 0, 1914, 0, 200, 0, 1794, 0, 2654, 0, 0, 25480, 420, 0, 2795, 0, 0, 229690, 0, 32559, 0, 0, 392, 253919, 0, 0, 0, 0, 379, 0, 0, 114, 0, 553, 10, 0, 1128, 0, 23610, 248, 151, 0, 418, 0, 651, 0, 36, 0, 0, 645, 0, 0, 513, 0, 0, 25480, 420, 34, 0, 0, 0, 15, 0, 3348, 0, 3496, 0, 35, 687, 0, 1, 48, 0, 0, 2803, 0, 0, 714, 1274, 0, 114, 62, 1006, 70268, 1200, 2357, 0, 497, 0, 497, 125, 0, 913, 4647, 3985, 0, 0, 3370, 245233, 0, 0, 687, 0, 4, 1288, 0, 0, 0, 0, 715, 0, 0, 687, 583, 0, 0, 1627, 0, 0, 11, 357, 1359, 0, 849, 0, 0, 1518, 462, 245233, 0, 0, 0, 0, 0, 0, 171, 70268, 0))\n"
     ]
    }
   ],
   "source": [
    "example_document = index.document(index.document_base())\n",
    "print(example_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a document consists of two things, a string representing the external document identifier and an integer list representing the identifiers of words that make up the document. Pyndri uses integer representations for words or terms, thus a token_id is an integer that represents a word whereas the token is the actual text of the word/term. Every id has a unique token and vice versa with the exception of stop words: words so common that there are uninformative, all of these receive the zero id.\n",
    "\n",
    "To see what some ids and their matching tokens we take a look at the dictionary of the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'new'), (2, 'percent'), (3, 'two'), (4, '1'), (5, 'people'), (6, 'million'), (7, '000'), (8, 'government'), (9, 'president'), (10, 'years'), (11, 'state'), (12, '2'), (13, 'states'), (14, 'three'), (15, 'time')]\n"
     ]
    }
   ],
   "source": [
    "token2id, id2token, _ = index.get_dictionary()\n",
    "print(list(id2token.items())[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this dictionary we can see the tokens for the (non-stop) words in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['52', 'students', 'arrested', 'takeover', 'university', 'massachusetts', 'building', 'fifty', 'two', 'students', 'arrested', 'tuesday', 'evening', 'occupying', 'university', 'massachusetts', 'building', 'overnight', 'protest', 'defense', 'department', 'funded', 'research', 'new', 'york', 'city', 'thousands', 'city', 'college', 'students', 'got', 'unscheduled', 'holiday', 'demonstrators', 'occupied', 'campus', 'administration', 'building', 'protest', 'possible', 'tuition', 'increases', 'prompting', 'officials', 'suspend', 'classes', '60', 'police', 'riot', 'gear', 'arrived', 'university', 'massachusetts', '5', 'p', 'm', 'two', 'hours', 'later', 'bus', 'drove', 'away', '29', 'students', 'camped', 'memorial', 'hall', 'students', 'charged', 'trespassing', '23', 'students', 'arrested', 'lying', 'bus', 'prevent', 'leaving', 'police', '300', 'students', 'stood', 'building', 'chanting', 'looking', 'students', 'hall', 'arrested', '35', 'students', 'occupied', 'memorial', 'hall', '1', 'p', 'm', 'monday', 'declined', 'offer', 'meet', 'administrators', 'provosts', 'office', 'tuesday', 'morning', 'presented', 'list', 'demands', 'halt', 'defense', 'department', 'research', '25', '000', 'student', 'campus', '40', 'students', 'left', 'building', 'tuesday', 'morning', 'university', 'administrators', 'told', 'arrested', '5', 'p', 'm', 'university', 'spokeswoman', 'jeanne', 'hopkins', 'takeover', 'second', 'western', 'massachusetts', 'campus', 'seven', 'protesters', 'arrested', 'april', '19', 'charges', 'disorderly', 'conduct', 'trespassing', 'demonstrating', 'military', 'funded', 'research', 'campus', 'particularly', 'research', 'anthrax', 'research', 'university', 'non', 'classified', 'researchers', 'make', 'work', 'public', 'university', 'rules', '11', '6', 'million', '22', 'percent', 'grant', 'money', 'received', 'university', 'came', 'defense', 'department', '1988', 'school', 'chancellor', 'joseph', 'd', 'duffey', 'issued', 'statement', 'telling', 'students', 'research', 'continue', 'campus', 'school', 'administrators', 'decide', 'differently', 'policy', 'negotiated', 'students', 'duffey', 'latest', 'occupation', 'began', 'students', 'rallying', 'monday', 'student', 'union', 'military', 'research', 'marched', 'administration', 'building', 'ducked', 'memorial', 'hall', 'en', 'route', 'followed', 'members', 'local', 'chapter', 'american', 'friends', 'service', 'committee', 'contended', 'research', 'dangerous', 'town', 'promotes', 'militarism', 'banned', 'university', 'argued', 'purpose', 'anthrax', 'research', 'peaceful', 'strain', 'bacteria', 'non', 'virulent', 'study', 'school', '23', 'years', 'incident', 'amherst', 'health', 'board', 'scheduled', 'hearing', 'wednesday', 'question', 'safety', 'anthrax', 'research', 'tuesday', 'time', '1969', 'classes', 'city', 'college', 'new', 'york', 'canceled', 'student', 'protests', 'school', 'spokesman', 'charles', 'deciccio', 'protesters', 'demanding', 'face', 'face', 'meeting', 'gov', 'mario', 'cuomo', 'feared', 'tuition', 'college', '1', '250', 'increased', 'college', 'staff', 'reduced', 'state', 'budget', 'cuts', 'governor', 'immediate', 'comment', 'tuition', 'set', 'deciccio']\n"
     ]
    }
   ],
   "source": [
    "print([id2token[word_id] for word_id in example_document[1] if word_id > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse can also be done, say we want to look for news about the \"University of Massachusetts\", the tokens of that query can be converted to ids using the reverse dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query by tokens: ['university', '', 'massachusetts']\n",
      "Query by ids with stopwords: [200, 0, 894]\n",
      "Query by ids without stopwords: [200, 894]\n"
     ]
    }
   ],
   "source": [
    "query_tokens = index.tokenize(\"University of Massachusetts\")\n",
    "print(\"Query by tokens:\", query_tokens)\n",
    "query_id_tokens = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "print(\"Query by ids with stopwords:\", query_id_tokens)\n",
    "query_id_tokens = [word_id for word_id in query_id_tokens if word_id > 0]\n",
    "print(\"Query by ids without stopwords:\", query_id_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally we can now match the document and query in the id space, let's see how often a word from the query occurs in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document AP890425-0001 has 13 word matches with query: \"university  massachusetts\".\n",
      "Document AP890425-0001 and query \"university  massachusetts\" have a 2.5% overlap.\n"
     ]
    }
   ],
   "source": [
    "matching_words = sum([True for word_id in example_document[1] if word_id in query_id_tokens])\n",
    "print(\"Document %s has %d word matches with query: \\\"%s\\\".\" % (example_document[0], matching_words, ' '.join(query_tokens)))\n",
    "print(\"Document %s and query \\\"%s\\\" have a %.01f%% overlap.\" % (example_document[0], ' '.join(query_tokens),matching_words/float(len(example_document[1]))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is certainly not everything Pyndri can do, it should give you an idea of how to use it. Please take a look at the [examples](https://github.com/cvangysel/pyndri) as it will help you a lot with this assignment.\n",
    "\n",
    "**CAUTION**: Avoid printing out the whole index in this Notebook as it will generate a lot of output and is likely to corrupt the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "close\n",
      "document\n",
      "document_base\n",
      "document_count\n",
      "document_ids\n",
      "document_length\n",
      "ext_document_id\n",
      "get_dictionary\n",
      "get_term_frequencies\n",
      "maximum_document\n",
      "process_term\n",
      "query\n",
      "term_count\n",
      "tokenize\n",
      "total_terms\n",
      "unique_terms\n"
     ]
    }
   ],
   "source": [
    "#Nichita:\n",
    "#No good documentation found. Let's see the methods and attributes of an Index object:\n",
    "methods = [method for method in dir(index) if '__' not in method and callable(getattr(index, method))]\n",
    "for method in methods:\n",
    "    print(method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the query file\n",
    "You can parse the query file (`ap_88_89/topics_title`) using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('51', 'Airbus Subsidies'), ('52', 'South African Sanctions'), ('53', 'Leveraged Buyouts'), ('54', 'Satellite Launch Contracts'), ('55', 'Insider Trading'), ('56', 'Prime (Lending) Rate Moves, Predictions'), ('57', 'MCI'), ('58', 'Rail Strikes'), ('59', 'Weather Related Fatalities'), ('60', 'Merit-Pay vs. Seniority'), ('61', 'Israeli Role in Iran-Contra Affair'), ('62', \"Military Coups D'etat\"), ('63', 'Machine Translation'), ('64', 'Hostage-Taking'), ('65', 'Information Retrieval Systems'), ('66', 'Natural Language Processing'), ('67', 'Politically Motivated Civil Disturbances'), ('68', 'Health Hazards from Fine-Diameter Fibers'), ('69', 'Attempts to Revive the SALT II Treaty'), ('70', 'Surrogate Motherhood'), ('71', 'Border Incursions'), ('72', 'Demographic Shifts in the U.S.'), ('73', 'Demographic Shifts across National Boundaries'), ('74', 'Conflicting Policy'), ('75', 'Automation'), ('76', 'U.S. Constitution - Original Intent'), ('77', 'Poaching'), ('78', 'Greenpeace'), ('79', 'FRG Political Party Positions'), ('80', '1988 Presidential Candidates Platforms'), ('81', 'Financial crunch for televangelists in the wake of the PTL scandal'), ('82', 'Genetic Engineering'), ('83', 'Measures to Protect the Atmosphere'), ('84', 'Alternative/renewable Energy Plant & Equipment Installation'), ('85', 'Official Corruption'), ('86', 'Bank Failures'), ('87', 'Criminal Actions Against Officers of Failed Financial Institutions'), ('88', 'Crude Oil Price Trends'), ('89', '\"Downstream\" Investments by OPEC Member States'), ('90', 'Data on Proven Reserves of Oil & Natural Gas Producers'), ('91', 'U.S. Army Acquisition of Advanced Weapons Systems'), ('92', 'International Military Equipment Sales'), ('93', 'What Backing Does the National Rifle Association Have?'), ('94', 'Computer-aided Crime'), ('95', 'Computer-aided Crime Detection'), ('96', 'Computer-Aided Medical Diagnosis'), ('97', 'Fiber Optics Applications'), ('98', 'Fiber Optics Equipment Manufacturers'), ('99', 'Iran-Contra Affair'), ('100', 'Controlling the Transfer of High Technology'), ('101', 'Design of the \"Star Wars\" Anti-missile Defense System'), ('102', \"Laser Research Applicable to the U.S.'s Strategic Defense Initiative\"), ('103', 'Welfare Reform'), ('104', 'Catastrophic Health Insurance'), ('105', '\"Black Monday\"'), ('106', 'U.S. Control of Insider Trading'), ('107', 'Japanese Regulation of Insider Trading'), ('108', 'Japanese Protectionist Measures'), ('109', 'Find Innovative Companies'), ('110', 'Black Resistance Against the South African Government'), ('111', 'Nuclear Proliferation'), ('112', 'Funding Biotechnology'), ('113', 'New Space Satellite Applications'), ('114', 'Non-commercial Satellite Launches'), ('115', 'Impact of the 1986 Immigration Law'), ('116', 'Generic Drug Substitutions'), ('117', 'Capacity of the U.S. Cellular Telephone Network'), ('118', 'International Terrorists'), ('119', 'Actions Against International Terrorists'), ('120', 'Economic Impact of International Terrorism'), ('121', 'Death from Cancer'), ('122', 'RDT&E of New Cancer Fighting Drugs'), ('123', 'Research into & Control of Carcinogens'), ('124', 'Alternatives to Traditional Cancer Therapies'), ('125', 'Anti-smoking Actions by Government'), ('126', 'Medical Ethics and Modern Technology'), ('127', 'U.S.-U.S.S.R. Arms Control Agreements'), ('128', 'Privatization of State Assets'), ('129', 'Soviet Spying on the U.S.'), ('130', 'Jewish Emigration and U.S.-USSR Relations'), ('131', 'McDonnell Douglas Contracts for Military Aircraft'), ('132', '\"Stealth\" Aircraft'), ('133', 'Hubble Space Telescope'), ('134', 'The Human Genome Project'), ('135', 'Possible Contributions of Gene Mapping to Medicine'), ('136', 'Diversification by Pacific Telesis'), ('137', 'Expansion in the U.S. Theme Park Industry'), ('138', 'Iranian Support for Lebanese Hostage-takers'), ('139', \"Iran's Islamic Revolution - Domestic and Foreign Social Consequences\"), ('140', 'Political Impact of Islamic Fundamentalism'), ('141', \"Japan's Handling of its Trade Surplus with the U.S.\"), ('142', 'Impact of Government Regulated Grain Farming on International Relations'), ('143', 'Why Protect U.S. Farmers?'), ('144', 'Management Problems at the United Nations'), ('145', 'Influence of the \"Pro-Israel Lobby\"'), ('146', 'Negotiating an End to the Nicaraguan Civil War'), ('147', 'Productivity Trends in the U.S. Economy'), ('148', 'Conflict in the Horn of Africa'), ('149', 'Industrial Espionage'), ('150', 'U.S. Political Campaign Financing'), ('151', 'Coping with overcrowded prisons'), ('152', 'Accusations of Cheating by Contractors on U.S. Defense Projects'), ('153', 'Insurance Coverage which pays for Long Term Care'), ('154', 'Oil Spills'), ('155', 'Right Wing Christian Fundamentalism in U.S.'), ('156', 'Efforts to enact Gun Control Legislation'), ('157', 'Causes and treatments of multiple sclerosis (MS)'), ('158', 'Term limitations for members of the U.S. Congress'), ('159', 'Electric Car Development'), ('160', 'Vitamins - The Cure for or Cause of Human Ailments'), ('161', 'Acid Rain'), ('162', 'Automobile Recalls'), ('163', 'Vietnam Veterans and Agent Orange'), ('164', 'Generic Drugs - Illegal Activities by Manufacturers'), ('165', 'Tobacco company advertising and the young'), ('166', 'Standardized testing and cultural bias'), ('167', 'Regulation of the showing of violence and explicit sex in motion picture theaters, on television, and on video cassettes.'), ('168', 'Financing AMTRAK'), ('169', 'Cost of Garbage/Trash Removal'), ('170', 'The Consequences of Implantation of Silicone Gel Breast Devices'), ('171', \"Use of Mutual Funds in an Individual's Retirement Strategy\"), ('172', 'The Effectiveness of Medical Products and Related Programs Utilized in the Cessation of Smoking.'), ('173', 'Smoking Bans'), ('174', 'Hazardous Waste Cleanup'), ('175', 'NRA Prevention of Gun Control Legislation'), ('176', 'Real-life private investigators'), ('177', 'English as the Official Language in U.S.'), ('178', 'Dog Maulings'), ('179', 'U. S. Restaurants in Foreign Lands'), ('180', 'Ineffectiveness of U.S. Embargoes/Sanctions'), ('181', 'Abuse of the Elderly by Family Members, and Medical and Nonmedical Personnel, and Initiatives Being Taken to Minimize This Mistreatment'), ('182', 'Commercial Overfishing Creates Food Fish Deficit'), ('183', 'Asbestos Related Lawsuits'), ('184', 'Corporate Pension Plans/Funds'), ('185', 'Reform of the U.S. Welfare System'), ('186', 'Difference of Learning Levels Among Inner City and More Suburban School Students'), ('187', 'Signs of the Demise of Independent Publishing'), ('188', 'Beachfront Erosion'), ('189', 'Real Motives for Murder'), ('190', 'Instances of Fraud Involving the Use of a Computer'), ('191', 'Efforts to Improve U.S. Schooling'), ('192', 'Oil Spill Cleanup'), ('193', 'Toys R Dangerous'), ('194', 'The Amount of Money Earned by Writers'), ('195', 'Stock Market Perturbations Attributable to Computer Initiated Trading'), ('196', 'School Choice Voucher System and its effects upon the entire U.S. educational program'), ('197', 'Reform of the jurisprudence system to stop juries from granting unreasonable monetary awards'), ('198', 'Gene Therapy and Its Benefits to Humankind'), ('199', 'Legality of Medically Assisted Suicides'), ('200', 'Impact of foreign textile imports on U.S. textile industry')])\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import io\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "def parse_topics(file_or_files,\n",
    "                 max_topics=sys.maxsize, delimiter=';'):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "\n",
    "    topics = collections.OrderedDict()\n",
    "\n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            topic_id, terms = line.split(delimiter, 1)\n",
    "\n",
    "            if topic_id in topics and (topics[topic_id] != terms):\n",
    "                    logging.error('Duplicate topic \"%s\" (%s vs. %s).',\n",
    "                                  topic_id,\n",
    "                                  topics[topic_id],\n",
    "                                  terms)\n",
    "\n",
    "            topics[topic_id] = terms\n",
    "\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                break\n",
    "\n",
    "    return topics\n",
    "\n",
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    print(parse_topics([f_topics]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement and compare lexical IR methods [35 points] ### \n",
    "\n",
    "In this task you will implement a number of lexical methods for IR using the **Pyndri** framework. Then you will evaluate these methods on the dataset we have provided using **TREC Eval**.\n",
    "\n",
    "Use the **Pyndri** framework to get statistics of the documents (term frequency, document frequency, collection frequency; **you are not allowed to use the query functionality of Pyndri**) and implement the following scoring methods in **Python**:\n",
    "\n",
    "- [TF-IDF](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html) and \n",
    "- [BM25](http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html) with k1=1.2 and b=0.75. **[5 points]**\n",
    "- Language models ([survey](https://drive.google.com/file/d/0B-zklbckv9CHc0c3b245UW90NE0/view))\n",
    "    - Jelinek-Mercer (explore different values of 𝛌 in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - Dirichlet Prior (explore different values of 𝛍 [500, 1000, 1500]). **[5 points]**\n",
    "    - Absolute discounting (explore different values of 𝛅 in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - [Positional Language Models](http://sifaka.cs.uiuc.edu/~ylv2/pub/sigir09-plm.pdf) define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of “soft” passage retrieval. Implement the PLM, all five kernels, but only the Best position strategy to score documents. Use 𝛔 equal to 50, and Dirichlet smoothing with 𝛍 optimized on the validation set (decide how to optimize this value yourself and motivate your decision in the report). **[10 points]**\n",
    "    \n",
    "Implement the above methods and report evaluation measures (on the test set) using the hyper parameter values you optimized on the validation set (also report the values of the hyper parameters). Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "For the language models, create plots showing `NDCG@10` with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's `subprocess`.\n",
    "\n",
    "Compute significance of the results using a [two-tailed paired Student t-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html) **[5 points]**. Be wary of false rejection of the null hypothesis caused by the [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem). There are multiple ways to mitigate this problem and it is up to you to choose one.\n",
    "\n",
    "Analyse the results by identifying specific queries where different methods succeed or fail and discuss possible reasons that cause these differences. This is *very important* in order to understand who the different retrieval functions behave.\n",
    "\n",
    "**NOTE**: Don’t forget to use log computations in your calculations to avoid underflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: You should structure your code around the helper functions we provide below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering statistics about 456 terms.\n",
      "Inverted index creation took 27.044995069503784 seconds.\n"
     ]
    }
   ],
   "source": [
    "with open('./ap_88_89/topics_title', 'r') as f_topics:\n",
    "    queries = parse_topics([f_topics])\n",
    "\n",
    "index = pyndri.Index('index/')\n",
    "\n",
    "num_documents = index.maximum_document() - index.document_base()\n",
    "\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "\n",
    "tokenized_queries = {\n",
    "    query_id: [dictionary.translate_token(token)\n",
    "               for token in index.tokenize(query_string)\n",
    "               if dictionary.has_token(token)]\n",
    "    for query_id, query_string in queries.items()}\n",
    "\n",
    "query_term_ids = set(\n",
    "    query_term_id\n",
    "    for query_term_ids in tokenized_queries.values()\n",
    "    for query_term_id in query_term_ids)\n",
    "\n",
    "print('Gathering statistics about', len(query_term_ids), 'terms.')\n",
    "\n",
    "# inverted index creation.\n",
    "start_time = time.time()\n",
    "\n",
    "document_lengths = {}\n",
    "unique_terms_per_document = {}\n",
    "\n",
    "inverted_index = defaultdict(dict)\n",
    "collection_frequencies = defaultdict(int)\n",
    "\n",
    "total_terms = 0\n",
    "\n",
    "for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "    ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "\n",
    "    document_bow = collections.Counter(\n",
    "        token_id for token_id in doc_token_ids\n",
    "        if token_id > 0)\n",
    "    document_length = sum(document_bow.values())\n",
    "\n",
    "    document_lengths[int_doc_id] = document_length\n",
    "    total_terms += document_length\n",
    "\n",
    "    unique_terms_per_document[int_doc_id] = len(document_bow)\n",
    "    \n",
    "    \n",
    "    for query_term_id in query_term_ids:\n",
    "        assert query_term_id is not None\n",
    "\n",
    "        document_term_frequency = document_bow.get(query_term_id, 0)\n",
    "\n",
    "        if document_term_frequency == 0:\n",
    "            continue\n",
    "\n",
    "        collection_frequencies[query_term_id] += document_term_frequency\n",
    "        inverted_index[query_term_id][int_doc_id] = document_term_frequency\n",
    "        \n",
    "avg_doc_length = total_terms / num_documents\n",
    "\n",
    "print('Inverted index creation took', time.time() - start_time, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_retrieval(model_name, score_fn, score_fn_params_dict):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example) \n",
    "    \"\"\"\n",
    "    run_out_path = '{}.run'.format(model_name)\n",
    "\n",
    "    if os.path.exists(run_out_path):\n",
    "        return\n",
    "\n",
    "    retrieval_start_time = time.time()\n",
    "\n",
    "    print('Retrieving using', model_name)\n",
    "\n",
    "    data = {}\n",
    "    \n",
    "    for query_id, query_token_list in tokenized_queries.items():\n",
    "        #for each query\n",
    "        \n",
    "        #print('query', [dictionary.id2token[x] for x in query_token_list])\n",
    "        query_docs_score_list = []\n",
    "        for int_doc_id in range(index.document_base(), index.maximum_document()):\n",
    "            #for each document\n",
    "            \n",
    "            ext_doc_id, doc_terms_ids = index.document(int_doc_id)\n",
    "            \n",
    "            q_d_score = multinomial_scoring_method_document_query(score_fn, score_fn_params_dict, int_doc_id, query_token_list)\n",
    "\n",
    "            query_docs_score_list.append((q_d_score, ext_doc_id))\n",
    "                \n",
    "        data[query_id] = query_docs_score_list\n",
    "        \n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multinomial_scoring_method_document_query(score_fn, score_fn_params_dict, int_doc_id, query_token_list):\n",
    "    #cosine similarity could be used between the two\n",
    "    #teacher suggested addition because it is more efficient\n",
    "    score = 0\n",
    "    \n",
    "    for token_id in query_token_list:\n",
    "        score_fn_params_dict['int_document_id'] = int_doc_id\n",
    "        score_fn_params_dict['query_term_id'] = token_id \n",
    "        score_fn_params_dict['document_term_freq'] = None\n",
    "        score += score_fn(**score_fn_params_dict)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf on all query-docs took: 0.00011610984802246094\n"
     ]
    }
   ],
   "source": [
    "def tfidf(int_document_id, query_term_id, document_term_freq):\n",
    "    \"\"\"\n",
    "    Scoring function for a document and a query term\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param document_term_freq: the document term frequency of the query term \n",
    "    \n",
    "    tf_idf(t;d)=log(1+tf(t;d)) * log(n/df(t))\n",
    "    former log expresses how much is d about t\n",
    "    latter log expresses inverse of how many documents are about t\n",
    "    used the log(1+tf) in order to dampen the result and to not overflow\n",
    "    tf is not normalized, but could be\n",
    "    \"\"\"\n",
    "    \n",
    "    #also is document_term_freq == inverted_index[query_term_id][int_document_id] ??\n",
    "    #I am using inverted_index[query_term_id][int_document_id] atm because I strongly think it is\n",
    "    if int_document_id not in inverted_index[query_term_id].keys():\n",
    "        tf = 0\n",
    "    else:\n",
    "        tf = inverted_index[query_term_id][int_document_id]\n",
    "    \n",
    "    if len(inverted_index[query_term_id]) == 0:\n",
    "        #making sure inverted index is never infinity\n",
    "        idf = 0\n",
    "    else:\n",
    "        idf = np.log2(num_documents)-np.log2(len(inverted_index[query_term_id]))\n",
    "    \n",
    "    score = np.log2(1 + tf) * idf\n",
    "    #if score != 0:\n",
    "    #    #print(score, tf, len(inverted_index[query_term_id]), dictionary.id2token[query_term_id])\n",
    "    return score\n",
    "\n",
    "# combining the three functions above:\n",
    "start_time = time.time()\n",
    "run_retrieval('tfidf', tfidf, {})\n",
    "print('tfidf on all query-docs took: ' + str(time.time()-start_time))\n",
    "# TODO implement the rest of the retrieval functions \n",
    "\n",
    "# TODO implement tools to help you with the analysis of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25 on all query-docs took: 8.463859558105469e-05\n"
     ]
    }
   ],
   "source": [
    "def bm25(int_document_id, query_term_id, document_term_freq, k1, b):\n",
    "    \"\"\"\n",
    "    Scoring function for a document and a query term\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param document_term_freq: the document term frequency of the query term \n",
    "    \n",
    "    bm25(t;d) = (k1+1)*tf(t;d) / (tf(t;d) + k1*((1-b) + b*(len(d)/avg_len_d)))\n",
    "    \n",
    "    – Most widely used weighting in IR\n",
    "    – Has TF, IDF, and document length components\n",
    "    – But only loosely inspired by probabilistic model\n",
    "    \"\"\"\n",
    "    \n",
    "    #also is document_term_freq == inverted_index[query_term_id][int_document_id] ??\n",
    "    #I am using inverted_index[query_term_id][int_document_id] atm because I strongly think it is\n",
    "    \n",
    "    if int_document_id not in inverted_index[query_term_id].keys():\n",
    "        tf = 0\n",
    "    else:\n",
    "        tf = inverted_index[query_term_id][int_document_id]\n",
    "    \n",
    "    score = (k1+1)*tf / (tf + k1*((1-b) + b*(document_lengths[int_document_id]/avg_doc_length)))\n",
    "    return score\n",
    "\n",
    "start_time = time.time()\n",
    "run_retrieval('bm25', bm25, {'k1':1.2, 'b':0.75})\n",
    "print('bm25 on all query-docs took: ' + str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language models: \n",
    "We are interested in estimating p(d|q) in order to get the most probable document, given a query\n",
    "\n",
    "$ p(d|q) = \\frac{p(q|d)p(d)}{p(q)} \\propto \\\\\n",
    "\\propto p(q|d)p(d) $\n",
    "\n",
    "Also, using the assumption that our prior on the documents is uniform\n",
    "\n",
    "$ p(q|d)p(d) = p(q|d) \\\\\n",
    "\\Rightarrow p(d|q) \\propto p(q|d) $\n",
    "\n",
    "Assuming a multinomial model for the query language model of the query, aka unigram:\n",
    "\n",
    "$p(q|d) = \\prod_{w_i \\in q} p(w_i|d)$\n",
    "\n",
    "We need to have the language model that generated d in order to be able to compute the above value:\n",
    "Each document, d has a distribution $\\theta_d$ which generated it.\n",
    "The equation now is:\n",
    "\n",
    "$p(q|d) = \\prod_{w_i \\in q} (w_i|\\theta_d)$\n",
    "\n",
    "We approximate $\\theta_d$ to the probability distribution most likely to have generated d\n",
    "\n",
    "$\\hat{\\theta_d} = \\underset{\\theta_d}{argmax}p(d|\\theta_d)$\n",
    "\n",
    "Given a multinomial assumption for $\\theta_d$\n",
    "\n",
    "$p(d|\\theta_d) $ $= \\underset{w \\in V}{\\prod}p(w|\\theta_d)^{tf(w;d)} \\\\\n",
    "                  = \\underset{w \\in d}{\\prod}p(w|\\theta_d)$\n",
    "\n",
    "Now by using this new $p(d|\\theta_d)$ and applying log we get the log likelihood: \n",
    "\n",
    "$ log p(d|\\theta_d) = \\underset{w \\in d}{\\sum}tf(w;d)p(w|\\theta_d)$\n",
    "\n",
    "In order to get the maximum likelihood we need to following a dual Lagrangian as we need to estimate $\\theta_d$ and keep the probability of words to sum to 1:\n",
    "\n",
    "$L = log p(d|\\theta_d) + \\lambda(1-\\underset{w \\in V}{\\sum}p(w|\\theta_d))$\n",
    "\n",
    "By taking the derivatives we get to:\n",
    "\n",
    "$p_{ml}(w|\\theta_d)=\\frac{tf(w; d)}{|d|}$\n",
    "\n",
    "Now the generative process of generating a query is:\n",
    "\n",
    "$ p(q|\\theta_d) = \\underset{w \\in q}{\\prod}{\\frac{tf(w; d)}{|d|}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing\n",
    "Smoothing means adjusting the ML estimation to avoid 0 probability and it is dont by taking mass from all the seen words and distributing it to the unseen words. This is similar to having a prior on unseen documents.\n",
    "\n",
    "### Additive smoothing\n",
    "Assumes that every word, even the unseen ones, have a small probability $\\epsilon$ <br>\n",
    "$p_\\epsilon(w|\\theta_d) = \\frac{tf(w;d)+\\epsilon}{|d|+\\epsilon V}$ <br>\n",
    "\n",
    "### Better smoothing..\n",
    "The probability of an unseen word should be proportional to the probability of a word given a background model.\n",
    "* language model estimated based on the entire document collection, C\n",
    "\n",
    "Estimate p(w|C):\n",
    "* words contributing equally: $ p(w|C) = \\frac{tf(w;C)}{|C|} $\n",
    "* documents contributing equally: $ p(w|C) = \\frac{1}{|C|} \\sum_{d \\in C}\\frac{tf(w;d)}{|d|} $\n",
    "* document frequency $ p(w|C) = \\frac{df(w)}{\\sum_{w' \\in V} df(w)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jelinek-Mercer smoothing, aka: Linearly interpolate with background language model ( $\\lambda$ ) <br>\n",
    "$ \\hat{p}_\\lambda(w|d) $ $= \\lambda p(w|d) + (1-\\lambda) p(w|C)\\\\\n",
    "= \\lambda \\frac{tf(w;d)}{|d|} + (1-\\lambda) \\frac{tf(w;C)}{|C|}$\n",
    "\n",
    "# TODO rerun language models as you were stupid and recommit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jelinek mercer query-docs took: 2.384185791015625e-05\n",
      "jelinek mercer query-docs took: 8.106231689453125e-06\n",
      "jelinek mercer query-docs took: 4.673004150390625e-05\n"
     ]
    }
   ],
   "source": [
    "def jelinek_mercer_smoothing(int_document_id, query_term_id, document_term_freq, lambda_jm):\n",
    "    \"\"\"\n",
    "    Scoring function for a document and a query term\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param document_term_freq: the document term frequency of the query term \n",
    "    \n",
    "    j_m(t;d) = lambda*tf(w;d)/|d| + (1-lambda)*tf(w;C)/|C|\n",
    "    \"\"\"\n",
    "    \n",
    "    #also is document_term_freq == inverted_index[query_term_id][int_document_id] ??\n",
    "    #I am using inverted_index[query_term_id][int_document_id] atm because I strongly think it is\n",
    "    \n",
    "    if int_document_id not in inverted_index[query_term_id].keys():\n",
    "        tf = 0\n",
    "    else:\n",
    "        tf = inverted_index[query_term_id][int_document_id]\n",
    "    \n",
    "    if query_term_id not in collection_frequencies.keys():\n",
    "        tf_collection = 0\n",
    "    else:\n",
    "        tf_collection = collection_frequencies[query_term_id]\n",
    "    \n",
    "    if document_lengths[int_document_id] == 0:\n",
    "        #when the document has nothing in it only the background model will influence score\n",
    "        tf = 0\n",
    "        doc_len = 1\n",
    "    else:\n",
    "        doc_len = document_lengths[int_document_id]\n",
    "    \n",
    "    score = lambda_jm*tf / doc_len + (1-lambda_jm)*tf_collection/total_terms\n",
    "    return score\n",
    "\n",
    "\n",
    "#try 0.1 0.5 0.9\n",
    "for lambda_jm in [0.1, 0.5, 0.9]:\n",
    "    start_time = time.time()\n",
    "    run_retrieval('jelinek_mercer:lambda='+str(lambda_jm), jelinek_mercer_smoothing, {'lambda_jm':lambda_jm})\n",
    "    print('jelinek mercer query-docs took: ' + str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute discounting smoothing ($\\delta$)\n",
    "IDEA:\n",
    "* Lower the probability of seen words by subtracting a constant from their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolute discounting all query-docs took: 2.2649765014648438e-05\n",
      "absolute discounting all query-docs took: 7.62939453125e-06\n",
      "absolute discounting all query-docs took: 5.9604644775390625e-06\n"
     ]
    }
   ],
   "source": [
    "def absolute_discounting(int_document_id, query_term_id, document_term_freq, delta):\n",
    "    \"\"\"\n",
    "    Scoring function for a document and a query term\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param document_term_freq: the document term frequency of the query term \n",
    "    \n",
    "    a_d(t;d) = max(tf(w;d)-delta, 0) / |d| + delta*|d_unique|/|d| * tf(w;C)/|C|\n",
    "    \"\"\"\n",
    "    \n",
    "    #also is document_term_freq == inverted_index[query_term_id][int_document_id] ??\n",
    "    #I am using inverted_index[query_term_id][int_document_id] atm because I strongly think it is\n",
    "    \n",
    "    if int_document_id not in inverted_index[query_term_id].keys():\n",
    "        tf = 0\n",
    "    else:\n",
    "        tf = inverted_index[query_term_id][int_document_id]\n",
    "    \n",
    "    if query_term_id not in collection_frequencies.keys():\n",
    "        tf_collection = 0\n",
    "    else:\n",
    "        tf_collection = collection_frequencies[query_term_id]\n",
    "    \n",
    "    if document_lengths[int_document_id] == 0:\n",
    "        #when the document has nothing in it only the background model will influence score\n",
    "        tf = delta\n",
    "        doc_len = 1\n",
    "    else:\n",
    "        doc_len = document_lengths[int_document_id]\n",
    "    unique_terms_count = unique_terms_per_document[int_document_id]\n",
    "    score = max(tf-delta,0)/doc_len + delta* unique_terms_count /doc_len*tf_collection/total_terms\n",
    "    return score\n",
    "\n",
    "for delta in [0.1, 0.5, 0.9]:\n",
    "    start_time = time.time()\n",
    "    run_retrieval('absolute_discounting:delta='+str(delta), absolute_discounting, {'delta':delta})\n",
    "    print('absolute discounting all query-docs took: ' + str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO COMMENT MULTINOMIAL DIRICHLET \n",
    "ALSO REASON for choosing p(w|C) = tf(w;C)/|C| everywhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirichlet prior on query-docs took: 3.647804260253906e-05\n",
      "dirichlet prior on query-docs took: 8.821487426757812e-06\n",
      "dirichlet prior on query-docs took: 2.3126602172851562e-05\n"
     ]
    }
   ],
   "source": [
    "def dirichlet_prior_smoothing(int_document_id, query_term_id, document_term_freq, mu):\n",
    "    \"\"\"\n",
    "    Scoring function for a document and a query term\n",
    "    \n",
    "    :param int_document_id: the document id\n",
    "    :param query_token_id: the query term id (assuming you have split the query to tokens)\n",
    "    :param document_term_freq: the document term frequency of the query term \n",
    "    \n",
    "    d(t;d) = (tf(w;d) + mu + tf(w;C)/|C|) / (|d| + mu)\n",
    "    \"\"\"\n",
    "    \n",
    "    #also is document_term_freq == inverted_index[query_term_id][int_document_id] ??\n",
    "    #I am using inverted_index[query_term_id][int_document_id] atm because I strongly think it is\n",
    "    if int_document_id not in inverted_index[query_term_id].keys():\n",
    "        tf = 0\n",
    "    else:\n",
    "        tf = inverted_index[query_term_id][int_document_id]\n",
    "    \n",
    "    if query_term_id not in collection_frequencies.keys():\n",
    "        tf_collection = 0\n",
    "    else:\n",
    "        tf_collection = collection_frequencies[query_term_id]\n",
    "    \n",
    "    doc_len = document_lengths[int_document_id]\n",
    "    \n",
    "    score = (tf+mu*tf_collection/total_terms ) / ( doc_len + mu )\n",
    "    return score\n",
    "\n",
    "\n",
    "for mu_value in [500, 1000, 1500]:\n",
    "    start_time = time.time()\n",
    "    run_retrieval('dirichlet_prior:mu='+str(mu_value), dirichlet_prior_smoothing, {'mu':mu_value})\n",
    "    print('dirichlet prior on query-docs took: ' + str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Language Models define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of “soft” passage retrieval. Implement the PLM, all five kernels, but only the Best position strategy to score documents. Use 𝛔 equal to 50, and Dirichlet smoothing with 𝛍 optimized on the validation set (decide how to optimize this value yourself and motivate your decision in the report). [10 points]\n",
    "\n",
    "\"3.2.1    Best Position Strategy\n",
    "Our first strategy is to simply score a document based on\n",
    "the score of its best matching position, formally,\n",
    "\n",
    "$S(q,d)=\\underset{i \\in [1,N]}{max}\\{S(q,d,i)\\}$\n",
    "\n",
    "This strategy resembles most existing studies on passage\n",
    "retrieval, which generally considered evidences from the best\n",
    "matching passage [4, 9, 16]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO finish PLM\n",
    "## Positional Language Model\n",
    "Defines a language model for each position of a document. Moreover, model should be a fuzzy description of the paragrpah, centered at the given position.\n",
    "\n",
    "term-frequency per position:\n",
    "\n",
    "$tf'(w,j;d)=\\sum_{i=0}^{|d|}tf(w,j;d)k(j,i) $ <br>\n",
    "where k is a non icreasing function describing the propagation of a term in it's vecinity <br>\n",
    "examples of k: <br>\n",
    "* A constant function: <br>\n",
    "$ k(i,j) $$= 1, |i-j| <= range \\\\\n",
    "           = 0, otherwise$ \n",
    "* or a Gaussian: <br>\n",
    "$ k(i,j)= exp \\left( \\frac{-(i-j)^2}{2\\sigma^2} \\right)$\n",
    "\n",
    "As a result we have the following language model per position: <br>\n",
    "\n",
    "$p(w|d,i) = \\frac{tf'(w,i;d)k(j,i)}{\\sum_{w' \\in V} tf'(w', i;d)} $\n",
    "\n",
    "This can result in multiple representations of d. As a result we design a per query score for each PLM:<br>\n",
    "$S(q,d,i)=-\\sum_{w \\in V}p(w|q)\\frac{p(w|q)}{p(w|d,i)}$, <br>\n",
    "where p(w|q) is an estimated query language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating top 100 doc to query dict took:1.3339924812316895\n"
     ]
    }
   ],
   "source": [
    "def get_tfifd_top_n_doc_query_pair(n):\n",
    "    dic = {}\n",
    "    \n",
    "    counter = 0\n",
    "    previous_query_id = -1\n",
    "    with open('tfidf.run','r') as fd:\n",
    "        for line in fd:\n",
    "            q_id, _, ext_doc_id = line.strip().split(' ')[0:3]\n",
    "            int_doc_id = index.document_ids([ext_doc_id])[0][1]\n",
    "            \n",
    "            if previous_query_id != q_id:\n",
    "                #if new query\n",
    "                previous_query_id = q_id\n",
    "                counter = 0\n",
    "\n",
    "            if counter < n:\n",
    "                #add it if it's in top n\n",
    "                if int_doc_id not in dic.keys():\n",
    "                     dic[int_doc_id] = []\n",
    "                dic[int_doc_id].append(q_id)\n",
    "            else:\n",
    "                #skip if not in top n\n",
    "                pass\n",
    "            counter += 1\n",
    "            \n",
    "    return dic\n",
    "\n",
    "start_time = time.time()\n",
    "top_n = 100\n",
    "doc_query_top_n_dic = get_tfifd_top_n_doc_query_pair(top_n)\n",
    "print('creating top ' + str(top_n) + ' doc to query dict took:' + str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def gaussian_cdf(i, sigma, N):\n",
    "    #AKA Z_i\n",
    "    #N is the document length\n",
    "    #sum_j in 1,N} k(j,i) = np.sqrt(2*np.pi*sigma*sigma) * (phi((N-i)/sigma) - phi((i-1)/sigma))\n",
    "    #phi = cummulative normal distribution \n",
    "    cdfs = norm.cdf((N-i)/sigma) - norm.cdf((1-i)/sigma)\n",
    "    if cdfs == 0:\n",
    "        print('ahoo its cdfs', norm.cdf((N-i)/sigma), N, i)\n",
    "    res = np.sqrt(2*np.pi*sigma*sigma) * (norm.cdf((N-i)/sigma) - norm.cdf((1-i)/sigma))\n",
    "    if res == 0:\n",
    "        print('HERE', sigma, N, i)\n",
    "    return res\n",
    "'''\n",
    "def gaussian_kernel(absolute_difference, sigma):\n",
    "    # absolute_difference = abs(i-j)\n",
    "    return np.exp( (-(absolute_difference)*(absolute_difference)) / (2*sigma*sigma) )\n",
    "\n",
    "# sigma*np.sqrt(2*np.pi) * Cumulative normal distribution function at x\n",
    "def GaussianCDF(x, mean, sigma):\n",
    "    res = 0\n",
    "    x=(x - mean) / sigma;\n",
    "    if x == 0:\n",
    "        res=0.5\n",
    "    else:\n",
    "        oor2pi = 1/(np.sqrt((2) * np.pi));\n",
    "        t = 1 / ((1) + 0.2316419 * abs(x))\n",
    "        t *= oor2pi * np.exp(-0.5 * x * x) \\\n",
    "             * (0.31938153   + t \\\n",
    "             * (-0.356563782 + t \\\n",
    "             * (1.781477937  + t \\\n",
    "             * (-1.821255978 + t * 1.330274429))))\n",
    "        if x >= 0:\n",
    "            res = 1 - t\n",
    "        else:\n",
    "            res = t\n",
    "    return sigma*np.sqrt(2*np.pi)*res\n",
    "\n",
    "\n",
    "def triangle_kernel(absolute_difference, sigma):\n",
    "    # absolute_difference = abs(i-j)\n",
    "    ans = 0\n",
    "    if absolute_difference <= sigma:\n",
    "        ans = 1 - absolute_difference / sigma\n",
    "    return ans\n",
    "\n",
    "#Cumulative distribution function of Triangle Kernel\n",
    "def TriangleCDF( x,  mean,  sigma):\n",
    "    res = 0\n",
    "    x = (x - mean) / sigma\n",
    "    if x >= 1:\n",
    "        res = sigma;\n",
    "    elif x < -1:\n",
    "        res = 0\n",
    "    elif x < 0:\n",
    "        res = sigma * (1 - abs(x)) * (1 - abs(x)) / 2.0\n",
    "    else:\n",
    "        res = sigma - sigma * (1 - x) * (1 - x) / 2.0\n",
    "    return res\n",
    "\n",
    "def cosine_kernel(absolute_difference, sigma):\n",
    "    # absolute_difference = abs(i-j)\n",
    "    ans = 0\n",
    "    if absolute_difference <= sigma:\n",
    "        ans = 1/2*(1+np.cos(absolute_difference*np.pi/sigma))\n",
    "    return ans\n",
    "\n",
    "#Cumulative distribution function of Cosine Kernel\n",
    "def CosineCDF( x,  mean,  sigma):\n",
    "    res = 0\n",
    "    x = (x - mean) / sigma\n",
    "    if x >= 1:\n",
    "        res = sigma\n",
    "    elif x < -1:\n",
    "        res = 0\n",
    "    elif x < 0:\n",
    "        res = sigma * (1 + x -  np.sin(np.pi * x) / np.pi) / 2.0\n",
    "    else:\n",
    "        res = sigma - sigma * (1 - x + np.sin(np.pi * x) / np.pi) / 2.0\n",
    "    return res\n",
    "\n",
    "def circle_kernel(absolute_difference, sigma):\n",
    "    # absolute_difference = abs(i-j)\n",
    "    ans = 0\n",
    "    if absolute_difference <= sigma:\n",
    "        ans = np.sqrt(1-(absolute_difference/sigma)*(absolute_difference/sigma))\n",
    "    return ans\n",
    "\n",
    "#Cumulative distribution function of Circle Kernel\n",
    "def CircleCDF( x,  mean,  sigma):\n",
    "    res = 0\n",
    "    x = (x - mean) / sigma\n",
    "    if x >= 1:\n",
    "        res = (np.pi - 2.0) * sigma\n",
    "    elif x < -1:\n",
    "        res = 0\n",
    "    elif x < 0:\n",
    "        res = sigma * (np.arcsin(x) + np.pi / 2.0 - np.sqrt(1 - x * x))\n",
    "    else:\n",
    "        res = (np.pi - 2.0) * sigma - sigma * (np.arcsin(-x) + np.pi / 2.0 - np.sqrt(1 - x * x))\n",
    "    return res\n",
    "\n",
    "def passage_kernel(absolute_difference, sigma):\n",
    "    # absolute_difference = abs(i-j)\n",
    "    ans = 0\n",
    "    if absolute_difference <= sigma:\n",
    "        ans = 1\n",
    "    return ans\n",
    "\n",
    "#Cumulative distribution function of Arc Kernel\n",
    "def ArcCDF( x,  mean,  sigma):\n",
    "    res = 0\n",
    "    x = (x - mean) / sigma\n",
    "    if x >= 1:\n",
    "        res = (np.pi - 1.0) * sigma / 2.0\n",
    "    elif x < -1:\n",
    "        res = 0\n",
    "    elif x < 0:\n",
    "        res = sigma * (np.arcsin(x) + np.pi / 2.0 - np.sqrt(1 - x * x) + (1 - abs(x)) * (1 - abs(x)) / 2.0) / 2.0\n",
    "    else:\n",
    "        res = (np.pi - 1.0) * sigma / 2.0 - sigma * (np.arcsin(-x) + np.pi / 2.0 - np.sqrt(1 - x * x) + (1 - abs(x)) * (1 - abs(x)) / 2.0) / 2.0\n",
    "    return res\n",
    "            \n",
    "def cdf(cdf_method, i,  sigma, N):\n",
    "    return cdf_method(N, i, sigma) - cdf_method(0, i, sigma)\n",
    "            \n",
    "def precompute_kernel(kernel_func, sigma):\n",
    "    max_len = max(document_lengths)+1\n",
    "    kernel = [0]*max_len\n",
    "    for i in range(max_len):\n",
    "        kernel[i] = kernel_func(absolute_difference=i, sigma=sigma)\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plm_retrieval(model_name, kernel_method, kernel_cdf_method, sigma, mu=0):\n",
    "    \"\"\"\n",
    "    mu = 0 means no dirichlet smoothing\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example) \n",
    "    \"\"\"\n",
    "    run_out_path = '{}.run'.format(model_name)\n",
    "\n",
    "    if os.path.exists(run_out_path):\n",
    "        return\n",
    "\n",
    "    retrieval_start_time = time.time()\n",
    "\n",
    "    print('Retrieving using', model_name)\n",
    "    data = {}\n",
    "    \n",
    "    kernel_at_diff = precompute_kernel(kernel_method, sigma)\n",
    "    \n",
    "    count_docs = 0\n",
    "    count_queries = 0\n",
    "    for int_doc_id in doc_query_top_n_dic.keys():\n",
    "        #for each document in the top 1000\n",
    "        \n",
    "        count_docs += 1\n",
    "            \n",
    "        if count_docs % 1000 == 0:\n",
    "            print(str(count_docs) + ' docs in: ' +str(time.time() - start_time))\n",
    "            \n",
    "        Z = [0]*document_lengths[int_doc_id]\n",
    "        cdf_params = {'cdf_method':kernel_cdf_method, 'i':0, 'sigma':sigma, 'N':document_lengths[int_doc_id]}\n",
    "        for i in range(document_lengths[int_doc_id]):\n",
    "            # Z_i = sum_{w' in V} tf'(w',i;d)\n",
    "            #for the gaussian kernel Z_i = cummulative_gaussian_kernel(i, sigma, N)\n",
    "            cdf_params['i'] = i\n",
    "            Z[i] = cdf(**cdf_params)\n",
    "        \n",
    "        for query_id in doc_query_top_n_dic[int_doc_id]:\n",
    "\n",
    "            query_token_list = tokenized_queries[query_id]\n",
    "            #for each query of the document in the top 1000    \n",
    "            ext_doc_id, doc_terms_ids = index.document(int_doc_id)\n",
    "            \n",
    "            q_d_score = score_plm_method_document_query(int_doc_id, query_token_list, Z, sigma, mu, kernel_at_diff)\n",
    "            \n",
    "            if query_id not in data.keys():\n",
    "                data[query_id] = []\n",
    "            data[query_id].append((q_d_score, ext_doc_id))\n",
    "        \n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_plm_method_document_query(int_doc_id,query_token_list, Z, sigma, mu, kernel_at_index_difference):\n",
    "    #S(Q,D) = max_{i in [1,N]} {S(Q,D,i)}\n",
    "    maximum = 0\n",
    "    for i in range(document_lengths[int_doc_id]):\n",
    "        #S(Q,D,i) = - sum_{w in query} (p(w|Q)*log(p(w|Q)/p(w|D,i))) \n",
    "        score = 0\n",
    "        for token_id in query_token_list:\n",
    "            #p(w|D,i))\n",
    "            doc_plm_at_i = positional_document_language_model(int_doc_id, i, Z[i], sigma, mu, kernel_at_index_difference, token_id)\n",
    "            #p(w|Q) = count(w in Q) / len(Q)\n",
    "            word_ml_query_model_estimate = query_token_list.count(token_id) / len(query_token_list)\n",
    "            \n",
    "            score += word_ml_query_model_estimate * np.log(word_ml_query_model_estimate / doc_plm_at_i)\n",
    "        score = score * (-1)\n",
    "        if score > maximum:\n",
    "            maximum = score\n",
    "    return score\n",
    "                            \n",
    "def positional_document_language_model(int_doc_id,  i, Z_i, sigma, mu, kernel_at_index_difference, token_id):\n",
    "    #p(w|d,i) = ( tf'(w,i;d) + mu*p(w|C) ) / ( sum_{w' in V} tf'(w',i;d) + mu )\n",
    "    if query_term_id not in collection_frequencies.keys():\n",
    "        tf_collection = 0\n",
    "    else:\n",
    "        tf_collection = collection_frequencies[query_term_id]\n",
    "    \n",
    "    #computing tf'\n",
    "    ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "    indices = [idx for idx, x in enumerate(doc_token_ids) if x == token_id]\n",
    "    tf_prime = 0\n",
    "    for idx in indices:\n",
    "        tf_prime += kernel_at_index_difference[abs(i-idx)]\n",
    "        \n",
    "    res = ( tf_prime + mu*tf_collection/total_terms)  / (Z_i + mu)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving using test2_plm_gaussian_kernel:sigma=50:mu=100\n",
      "1000 docs in: 122.04963278770447\n",
      "2000 docs in: 229.45867466926575\n",
      "3000 docs in: 354.8099286556244\n",
      "4000 docs in: 473.33420515060425\n",
      "5000 docs in: 572.0266718864441\n",
      "6000 docs in: 679.2471206188202\n",
      "7000 docs in: 784.3018388748169\n",
      "8000 docs in: 893.2038757801056\n",
      "9000 docs in: 1003.2750158309937\n",
      "10000 docs in: 1114.1426339149475\n",
      "11000 docs in: 1217.6044671535492\n",
      "plm gaussian_kernel on all query docs took: 1294.124186515808\n",
      "Retrieving using test2_plm_gaussian_kernel:sigma=50:mu=500\n",
      "1000 docs in: 130.83012557029724\n",
      "2000 docs in: 237.73094058036804\n",
      "3000 docs in: 364.7574737071991\n",
      "4000 docs in: 485.80920243263245\n",
      "5000 docs in: 592.1625292301178\n",
      "6000 docs in: 707.2590334415436\n",
      "7000 docs in: 820.6258080005646\n",
      "8000 docs in: 933.1621844768524\n",
      "9000 docs in: 1052.1746850013733\n",
      "10000 docs in: 1167.3287465572357\n",
      "11000 docs in: 1276.0370919704437\n",
      "plm gaussian_kernel on all query docs took: 1358.0194358825684\n",
      "Retrieving using test2_plm_gaussian_kernel:sigma=50:mu=1000\n",
      "1000 docs in: 122.90289640426636\n",
      "2000 docs in: 228.8808948993683\n",
      "3000 docs in: 354.46573781967163\n",
      "4000 docs in: 478.6960759162903\n",
      "5000 docs in: 582.1085052490234\n",
      "6000 docs in: 695.5463559627533\n",
      "7000 docs in: 806.6030945777893\n",
      "8000 docs in: 920.5772545337677\n",
      "9000 docs in: 1037.9613897800446\n",
      "10000 docs in: 1154.34348654747\n",
      "11000 docs in: 1261.0627751350403\n",
      "plm gaussian_kernel on all query docs took: 1338.085110425949\n",
      "Retrieving using test2_plm_gaussian_kernel:sigma=50:mu=1500\n",
      "1000 docs in: 122.46115136146545\n",
      "2000 docs in: 227.6091639995575\n",
      "3000 docs in: 355.16733622550964\n",
      "4000 docs in: 477.7827854156494\n",
      "5000 docs in: 582.3912811279297\n",
      "6000 docs in: 692.6306796073914\n",
      "7000 docs in: 804.107165813446\n",
      "8000 docs in: 917.423816204071\n",
      "9000 docs in: 1032.7507808208466\n",
      "10000 docs in: 1149.246562242508\n",
      "11000 docs in: 1259.7818019390106\n",
      "plm gaussian_kernel on all query docs took: 1339.7572638988495\n",
      "Retrieving using test2_plm_cosine_kernel:sigma=50:mu=100\n",
      "1000 docs in: 117.80954670906067\n",
      "2000 docs in: 223.47443866729736\n",
      "3000 docs in: 348.5305588245392\n",
      "4000 docs in: 460.99637269973755\n",
      "5000 docs in: 558.1267464160919\n",
      "6000 docs in: 664.224856376648\n",
      "7000 docs in: 768.8712782859802\n",
      "8000 docs in: 877.1799898147583\n",
      "9000 docs in: 986.7751975059509\n",
      "10000 docs in: 1097.5617005825043\n",
      "11000 docs in: 1201.4620423316956\n",
      "plm cosine_kernel on all query docs took: 1276.8719744682312\n",
      "Retrieving using test2_plm_cosine_kernel:sigma=50:mu=500\n",
      "1000 docs in: 115.66084361076355\n",
      "2000 docs in: 214.28811478614807\n",
      "3000 docs in: 331.70193243026733\n",
      "4000 docs in: 444.3802306652069\n",
      "5000 docs in: 541.3543374538422\n",
      "6000 docs in: 646.7991480827332\n",
      "7000 docs in: 749.9612019062042\n",
      "8000 docs in: 856.4879379272461\n",
      "9000 docs in: 964.1956887245178\n",
      "10000 docs in: 1072.8894355297089\n",
      "11000 docs in: 1174.8843717575073\n",
      "plm cosine_kernel on all query docs took: 1250.5046985149384\n",
      "Retrieving using test2_plm_cosine_kernel:sigma=50:mu=1000\n",
      "1000 docs in: 115.56054711341858\n",
      "2000 docs in: 214.08429670333862\n",
      "3000 docs in: 331.5627703666687\n",
      "4000 docs in: 445.45302081108093\n",
      "5000 docs in: 547.2125084400177\n",
      "6000 docs in: 658.1018540859222\n",
      "7000 docs in: 761.6290857791901\n",
      "8000 docs in: 868.2310461997986\n",
      "9000 docs in: 974.9391031265259\n",
      "10000 docs in: 1082.3755118846893\n",
      "11000 docs in: 1183.248125076294\n",
      "plm cosine_kernel on all query docs took: 1258.039805650711\n",
      "Retrieving using test2_plm_cosine_kernel:sigma=50:mu=1500\n",
      "1000 docs in: 114.49769186973572\n",
      "2000 docs in: 212.32451009750366\n",
      "3000 docs in: 328.369686126709\n",
      "4000 docs in: 439.84383034706116\n",
      "5000 docs in: 536.954508304596\n",
      "6000 docs in: 647.7930817604065\n",
      "7000 docs in: 754.0286204814911\n",
      "8000 docs in: 863.3024535179138\n",
      "9000 docs in: 973.5338978767395\n",
      "10000 docs in: 1088.4003114700317\n",
      "11000 docs in: 1192.128411769867\n",
      "plm cosine_kernel on all query docs took: 1269.8228125572205\n",
      "Retrieving using test2_plm_circle_kernel:sigma=50:mu=100\n",
      "1000 docs in: 116.38598918914795\n",
      "2000 docs in: 213.69370651245117\n",
      "3000 docs in: 330.2616677284241\n",
      "4000 docs in: 442.81971645355225\n",
      "5000 docs in: 542.9456198215485\n",
      "6000 docs in: 656.1575040817261\n",
      "7000 docs in: 762.0268995761871\n",
      "8000 docs in: 871.3758680820465\n",
      "9000 docs in: 982.6665720939636\n",
      "10000 docs in: 1094.5709171295166\n",
      "11000 docs in: 1202.3337097167969\n",
      "plm circle_kernel on all query docs took: 1279.3157176971436\n",
      "Retrieving using test2_plm_circle_kernel:sigma=50:mu=500\n",
      "1000 docs in: 127.22346758842468\n",
      "2000 docs in: 233.69856929779053\n",
      "3000 docs in: 355.9690103530884\n",
      "4000 docs in: 475.1632616519928\n",
      "5000 docs in: 577.7721693515778\n",
      "6000 docs in: 689.4238007068634\n",
      "7000 docs in: 799.1866574287415\n",
      "8000 docs in: 910.697695016861\n",
      "9000 docs in: 1028.857296705246\n",
      "10000 docs in: 1144.887683391571\n",
      "11000 docs in: 1255.7126123905182\n",
      "plm circle_kernel on all query docs took: 1331.866607427597\n",
      "Retrieving using test2_plm_circle_kernel:sigma=50:mu=1000\n",
      "1000 docs in: 115.39072513580322\n",
      "2000 docs in: 213.3082823753357\n",
      "3000 docs in: 329.61825704574585\n",
      "4000 docs in: 441.5851991176605\n",
      "5000 docs in: 538.1611301898956\n",
      "6000 docs in: 648.9924945831299\n",
      "7000 docs in: 758.5266556739807\n",
      "8000 docs in: 872.714168548584\n",
      "9000 docs in: 984.8187370300293\n",
      "10000 docs in: 1098.5323994159698\n",
      "11000 docs in: 1201.7213933467865\n",
      "plm circle_kernel on all query docs took: 1281.391052722931\n",
      "Retrieving using test2_plm_circle_kernel:sigma=50:mu=1500\n",
      "1000 docs in: 116.31507706642151\n",
      "2000 docs in: 218.21430921554565\n",
      "3000 docs in: 342.4695498943329\n",
      "4000 docs in: 459.54997205734253\n",
      "5000 docs in: 560.9932932853699\n",
      "6000 docs in: 677.3304703235626\n",
      "7000 docs in: 787.003021478653\n",
      "8000 docs in: 902.1197919845581\n",
      "9000 docs in: 1019.3299942016602\n",
      "10000 docs in: 1134.5372986793518\n",
      "11000 docs in: 1240.2748107910156\n",
      "plm circle_kernel on all query docs took: 1319.3446640968323\n",
      "Retrieving using test2_plm_passage_kernel:sigma=50:mu=100\n",
      "1000 docs in: 114.43194222450256\n",
      "2000 docs in: 216.64231634140015\n",
      "3000 docs in: 336.75336599349976\n",
      "4000 docs in: 452.53502798080444\n",
      "5000 docs in: 555.3379371166229\n",
      "6000 docs in: 665.5384225845337\n",
      "7000 docs in: 771.6588358879089\n",
      "8000 docs in: 886.0618364810944\n",
      "9000 docs in: 997.5236492156982\n",
      "10000 docs in: 1109.1383519172668\n",
      "11000 docs in: 1216.0343627929688\n",
      "plm passage_kernel on all query docs took: 1294.624585866928\n",
      "Retrieving using test2_plm_passage_kernel:sigma=50:mu=500\n",
      "1000 docs in: 129.18743705749512\n",
      "2000 docs in: 236.15593194961548\n",
      "3000 docs in: 360.20557284355164\n",
      "4000 docs in: 476.1457488536835\n",
      "5000 docs in: 578.2567572593689\n",
      "6000 docs in: 690.4806017875671\n",
      "7000 docs in: 800.3234269618988\n",
      "8000 docs in: 913.8827097415924\n",
      "9000 docs in: 1022.3240187168121\n",
      "10000 docs in: 1133.860514163971\n",
      "11000 docs in: 1236.8526186943054\n",
      "plm passage_kernel on all query docs took: 1312.9301226139069\n",
      "Retrieving using test2_plm_passage_kernel:sigma=50:mu=1000\n",
      "1000 docs in: 121.03867435455322\n",
      "2000 docs in: 221.91368651390076\n",
      "3000 docs in: 341.30573892593384\n",
      "4000 docs in: 471.1631042957306\n",
      "5000 docs in: 575.8524146080017\n",
      "6000 docs in: 690.3170495033264\n",
      "7000 docs in: 807.9669234752655\n",
      "8000 docs in: 921.2256860733032\n",
      "9000 docs in: 1028.888620376587\n",
      "10000 docs in: 1136.79847240448\n",
      "11000 docs in: 1245.8644626140594\n",
      "plm passage_kernel on all query docs took: 1323.5061242580414\n",
      "Retrieving using test2_plm_passage_kernel:sigma=50:mu=1500\n",
      "1000 docs in: 117.76151823997498\n",
      "2000 docs in: 218.35328769683838\n",
      "3000 docs in: 337.79395604133606\n",
      "4000 docs in: 449.46424317359924\n",
      "5000 docs in: 545.4931857585907\n",
      "6000 docs in: 649.8758661746979\n",
      "7000 docs in: 752.1249947547913\n",
      "8000 docs in: 858.180202960968\n",
      "9000 docs in: 964.6761593818665\n",
      "10000 docs in: 1072.0802719593048\n",
      "11000 docs in: 1172.854617357254\n",
      "plm passage_kernel on all query docs took: 1247.4150240421295\n",
      "Retrieving using test2_plm_triangle_kernel:sigma=50:mu=100\n",
      "1000 docs in: 113.42804789543152\n",
      "2000 docs in: 210.06444716453552\n",
      "3000 docs in: 324.8509659767151\n",
      "4000 docs in: 435.57499623298645\n",
      "5000 docs in: 531.0018651485443\n",
      "6000 docs in: 634.3082005977631\n",
      "7000 docs in: 735.519727230072\n",
      "8000 docs in: 849.3897802829742\n",
      "9000 docs in: 964.0918998718262\n",
      "10000 docs in: 1072.4620661735535\n",
      "11000 docs in: 1174.0450863838196\n",
      "plm triangle_kernel on all query docs took: 1248.9908564090729\n",
      "Retrieving using test2_plm_triangle_kernel:sigma=50:mu=500\n",
      "1000 docs in: 116.79968070983887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 docs in: 214.3794186115265\n",
      "3000 docs in: 329.7280514240265\n",
      "4000 docs in: 442.4698996543884\n",
      "5000 docs in: 537.9499745368958\n",
      "6000 docs in: 641.5327463150024\n",
      "7000 docs in: 742.9599940776825\n",
      "8000 docs in: 849.833687543869\n",
      "9000 docs in: 960.5624656677246\n",
      "10000 docs in: 1090.4291679859161\n",
      "11000 docs in: 1208.5101642608643\n",
      "plm triangle_kernel on all query docs took: 1290.7984714508057\n",
      "Retrieving using test2_plm_triangle_kernel:sigma=50:mu=1000\n",
      "1000 docs in: 123.88587093353271\n",
      "2000 docs in: 230.79310584068298\n",
      "3000 docs in: 352.239541053772\n",
      "4000 docs in: 474.08543610572815\n",
      "5000 docs in: 575.6894443035126\n",
      "6000 docs in: 689.5793523788452\n",
      "7000 docs in: 796.7322874069214\n",
      "8000 docs in: 908.5812227725983\n",
      "9000 docs in: 1021.8311941623688\n",
      "10000 docs in: 1137.7258517742157\n",
      "11000 docs in: 1243.0424287319183\n",
      "plm triangle_kernel on all query docs took: 1321.9400098323822\n",
      "Retrieving using test2_plm_triangle_kernel:sigma=50:mu=1500\n",
      "1000 docs in: 115.62067461013794\n",
      "2000 docs in: 212.89122867584229\n",
      "3000 docs in: 328.0618999004364\n",
      "4000 docs in: 440.5357530117035\n",
      "5000 docs in: 540.8812549114227\n",
      "6000 docs in: 648.565268278122\n",
      "7000 docs in: 751.0056042671204\n",
      "8000 docs in: 859.4576869010925\n",
      "9000 docs in: 978.1276521682739\n",
      "10000 docs in: 1088.1667175292969\n",
      "11000 docs in: 1188.0147378444672\n",
      "plm triangle_kernel on all query docs took: 1261.8715806007385\n"
     ]
    }
   ],
   "source": [
    "pairs = [(gaussian_kernel, GaussianCDF), (cosine_kernel, CosineCDF), (circle_kernel, CircleCDF), (passage_kernel, ArcCDF), (triangle_kernel,TriangleCDF)]\n",
    "sigma_value = 50\n",
    "for pair in pairs:\n",
    "    for mu_value in [100,500,1000,1500]:\n",
    "        start_time = time.time()\n",
    "        plm_retrieval('test2_plm_'+pair[0].__name__+':sigma='+str(sigma_value)+':mu='+str(mu_value), pair[0], pair[1], sigma=sigma_value,mu=mu_value)\n",
    "        print('plm '+pair[0].__name__+' on all query docs took: ' + str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "map                   \tall\t0.2403\n",
      " \n",
      "map                   \tall\t0.2196\n",
      " \n",
      "map                   \tall\t0.1366\n",
      "map                   \tall\t0.1366\n",
      "map                   \tall\t0.1366\n",
      " \n",
      "map                   \tall\t0.1367\n",
      "map                   \tall\t0.1404\n",
      "map                   \tall\t0.1397\n",
      " \n",
      "map                   \tall\t0.1758\n",
      "map                   \tall\t0.1733\n",
      "map                   \tall\t0.1725\n",
      " \n",
      "map                   \tall\t0.0998\n",
      "map                   \tall\t0.1534\n",
      "map                   \tall\t0.1528\n",
      "map                   \tall\t0.1525\n",
      "map                   \tall\t0.1523\n",
      " \n",
      "map                   \tall\t0.1245\n",
      "map                   \tall\t0.1245\n",
      "map                   \tall\t0.1245\n",
      "map                   \tall\t0.1247\n",
      " \n",
      "map                   \tall\t0.1218\n",
      "map                   \tall\t0.1216\n",
      "map                   \tall\t0.1216\n",
      "map                   \tall\t0.1214\n",
      " \n",
      "map                   \tall\t0.1220\n",
      "map                   \tall\t0.1220\n",
      "map                   \tall\t0.1220\n",
      "map                   \tall\t0.1220\n",
      " \n",
      "map                   \tall\t0.1230\n",
      "map                   \tall\t0.1231\n",
      "map                   \tall\t0.1230\n",
      "map                   \tall\t0.1229\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo ' '\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './tfidf.run' | grep -E \"^map\\s.*all\"\n",
    "echo ' '\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './bm25.run' | grep -E \"^map\\s.*all\"\n",
    "echo ' '\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './jelinek_mercer:lambda=0.1.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './jelinek_mercer:lambda=0.5.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './jelinek_mercer:lambda=0.9.run' | grep -E \"^map\\s.*all\"\n",
    "echo ' '\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './absolute_discounting:delta=0.1.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './absolute_discounting:delta=0.5.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './absolute_discounting:delta=0.9.run' | grep -E \"^map\\s.*all\"\n",
    "echo ' '\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './dirichlet_prior:mu=500.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './dirichlet_prior:mu=1000.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './dirichlet_prior:mu=1500.run' | grep -E \"^map\\s.*all\"\n",
    "echo ' '\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_gaussian_kernel:sigma=50:mu=0.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_gaussian_kernel:sigma=50:mu=100.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_gaussian_kernel:sigma=50:mu=500.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_gaussian_kernel:sigma=50:mu=1000.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_gaussian_kernel:sigma=50:mu=1500.run' | grep -E \"^map\\s.*all\"\n",
    "echo ' '\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_circle_kernel:sigma=50:mu=100.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_circle_kernel:sigma=50:mu=500.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_circle_kernel:sigma=50:mu=1000.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_circle_kernel:sigma=50:mu=1500.run' | grep -E \"^map\\s.*all\"\n",
    "echo ' '\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_cosine_kernel:sigma=50:mu=100.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_cosine_kernel:sigma=50:mu=500.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_cosine_kernel:sigma=50:mu=1000.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_cosine_kernel:sigma=50:mu=1500.run' | grep -E \"^map\\s.*all\"\n",
    "echo ' '\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_passage_kernel:sigma=50:mu=100.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_passage_kernel:sigma=50:mu=500.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_passage_kernel:sigma=50:mu=1000.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_passage_kernel:sigma=50:mu=1500.run' | grep -E \"^map\\s.*all\"\n",
    "echo ' '\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_triangle_kernel:sigma=50:mu=100.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_triangle_kernel:sigma=50:mu=500.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_triangle_kernel:sigma=50:mu=1000.run' | grep -E \"^map\\s.*all\"\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_triangle_kernel:sigma=50:mu=1500.run' | grep -E \"^map\\s.*all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map                   \tall\t0.1354\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_test' './plm_gaussian_kernel:sigma=50:mu=100.run' | grep -E \"^map\\s.*all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 P_100 P_100 P_100 P_100\n",
      "1 103 103 103 103\n",
      "2 0.3000 0.3000 0.3000 0.3000\n",
      "3 recall_100 recall_100 recall_100 recall_100\n",
      "4 103 103 103 103\n",
      "5 0.5455 0.5455 0.5455 0.5455\n",
      "6 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "7 103 103 103 103\n",
      "8 0.4171 0.4171 0.4171 0.4171\n",
      "9 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "10 103 103 103 103\n",
      "11 0.1510 0.1510 0.1510 0.1510\n",
      "12 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "13 103 103 103 103\n",
      "14 0.5455 0.5455 0.5455 0.5455\n",
      "15 P_100 P_100 P_100 P_100\n",
      "16 111 111 111 111\n",
      "17 0.7000 0.7000 0.7000 0.7000\n",
      "18 recall_100 recall_100 recall_100 recall_100\n",
      "19 111 111 111 111\n",
      "20 0.6250 0.6250 0.6250 0.6250\n",
      "21 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "22 111 111 111 111\n",
      "23 0.7161 0.7161 0.7161 0.7161\n",
      "24 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "25 111 111 111 111\n",
      "26 0.4611 0.4611 0.4611 0.4611\n",
      "27 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "28 111 111 111 111\n",
      "29 0.7000 0.7000 0.7000 0.7000\n",
      "30 P_100 P_100 P_100 P_100\n",
      "31 114 114 114 114\n",
      "32 0.3200 0.3200 0.3200 0.3200\n",
      "33 recall_100 recall_100 recall_100 recall_100\n",
      "34 114 114 114 114\n",
      "35 0.2602 0.2602 0.2602 0.2602\n",
      "36 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "37 114 114 114 114\n",
      "38 0.2726 0.2726 0.2726 0.2726\n",
      "39 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "40 114 114 114 114\n",
      "41 0.0649 0.0649 0.0649 0.0649\n",
      "42 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "43 114 114 114 114\n",
      "44 0.3200 0.3200 0.3200 0.3200\n",
      "45 P_100 P_100 P_100 P_100\n",
      "46 120 120 120 120\n",
      "47 0.0300 0.0300 0.0300 0.0300\n",
      "48 recall_100 recall_100 recall_100 recall_100\n",
      "49 120 120 120 120\n",
      "50 0.0750 0.0750 0.0750 0.0750\n",
      "51 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "52 120 120 120 120\n",
      "53 0.0584 0.0584 0.0584 0.0584\n",
      "54 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "55 120 120 120 120\n",
      "56 0.0051 0.0051 0.0051 0.0051\n",
      "57 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "58 120 120 120 120\n",
      "59 0.0750 0.0750 0.0750 0.0750\n",
      "60 P_100 P_100 P_100 P_100\n",
      "61 123 123 123 123\n",
      "62 0.0900 0.0900 0.0900 0.0900\n",
      "63 recall_100 recall_100 recall_100 recall_100\n",
      "64 123 123 123 123\n",
      "65 0.1286 0.1286 0.1286 0.1286\n",
      "66 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "67 123 123 123 123\n",
      "68 0.1039 0.1039 0.1039 0.1039\n",
      "69 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "70 123 123 123 123\n",
      "71 0.0122 0.0122 0.0122 0.0122\n",
      "72 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "73 123 123 123 123\n",
      "74 0.1286 0.1286 0.1286 0.1286\n",
      "75 P_100 P_100 P_100 P_100\n",
      "76 135 135 135 135\n",
      "77 0.3900 0.3900 0.3900 0.3900\n",
      "78 recall_100 recall_100 recall_100 recall_100\n",
      "79 135 135 135 135\n",
      "80 0.5200 0.5200 0.5200 0.5200\n",
      "81 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "82 135 135 135 135\n",
      "83 0.5371 0.5371 0.5371 0.5371\n",
      "84 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "85 135 135 135 135\n",
      "86 0.2622 0.2622 0.2622 0.2622\n",
      "87 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "88 135 135 135 135\n",
      "89 0.5200 0.5200 0.5200 0.5200\n",
      "90 P_100 P_100 P_100 P_100\n",
      "91 143 143 143 143\n",
      "92 0.1200 0.1200 0.1200 0.1200\n",
      "93 recall_100 recall_100 recall_100 recall_100\n",
      "94 143 143 143 143\n",
      "95 0.0443 0.0443 0.0443 0.0443\n",
      "96 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "97 143 143 143 143\n",
      "98 0.1450 0.1450 0.1450 0.1450\n",
      "99 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "100 143 143 143 143\n",
      "101 0.0092 0.0092 0.0092 0.0092\n",
      "102 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "103 143 143 143 143\n",
      "104 0.1200 0.1200 0.1200 0.1200\n",
      "105 P_100 P_100 P_100 P_100\n",
      "106 144 144 144 144\n",
      "107 0.0700 0.0700 0.0700 0.0700\n",
      "108 recall_100 recall_100 recall_100 recall_100\n",
      "109 144 144 144 144\n",
      "110 0.1667 0.1667 0.1667 0.1667\n",
      "111 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "112 144 144 144 144\n",
      "113 0.1158 0.1158 0.1158 0.1158\n",
      "114 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "115 144 144 144 144\n",
      "116 0.0137 0.0137 0.0137 0.0137\n",
      "117 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "118 144 144 144 144\n",
      "119 0.1667 0.1667 0.1667 0.1667\n",
      "120 P_100 P_100 P_100 P_100\n",
      "121 151 151 151 151\n",
      "122 0.2300 0.2300 0.2300 0.2300\n",
      "123 recall_100 recall_100 recall_100 recall_100\n",
      "124 151 151 151 151\n",
      "125 0.2875 0.2875 0.2875 0.2875\n",
      "126 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "127 151 151 151 151\n",
      "128 0.2509 0.2509 0.2509 0.2509\n",
      "129 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "130 151 151 151 151\n",
      "131 0.0600 0.0600 0.0600 0.0600\n",
      "132 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "133 151 151 151 151\n",
      "134 0.2875 0.2875 0.2875 0.2875\n",
      "135 P_100 P_100 P_100 P_100\n",
      "136 155 155 155 155\n",
      "137 0.0000 0.0000 0.0000 0.0000\n",
      "138 recall_100 recall_100 recall_100 recall_100\n",
      "139 155 155 155 155\n",
      "140 0.0000 0.0000 0.0000 0.0000\n",
      "141 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "142 155 155 155 155\n",
      "143 0.0000 0.0000 0.0000 0.0000\n",
      "144 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "145 155 155 155 155\n",
      "146 0.0000 0.0000 0.0000 0.0000\n",
      "147 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "148 155 155 155 155\n",
      "149 0.0000 0.0000 0.0000 0.0000\n",
      "150 P_100 P_100 P_100 P_100\n",
      "151 158 158 158 158\n",
      "152 0.0000 0.0000 0.0000 0.0000\n",
      "153 recall_100 recall_100 recall_100 recall_100\n",
      "154 158 158 158 158\n",
      "155 0.0000 0.0000 0.0000 0.0000\n",
      "156 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "157 158 158 158 158\n",
      "158 0.0000 0.0000 0.0000 0.0000\n",
      "159 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "160 158 158 158 158\n",
      "161 0.0000 0.0000 0.0000 0.0000\n",
      "162 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "163 158 158 158 158\n",
      "164 0.0000 0.0000 0.0000 0.0000\n",
      "165 P_100 P_100 P_100 P_100\n",
      "166 165 165 165 165\n",
      "167 0.1000 0.1000 0.1000 0.1000\n",
      "168 recall_100 recall_100 recall_100 recall_100\n",
      "169 165 165 165 165\n",
      "170 0.4000 0.4000 0.4000 0.4000\n",
      "171 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "172 165 165 165 165\n",
      "173 0.2439 0.2439 0.2439 0.2439\n",
      "174 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "175 165 165 165 165\n",
      "176 0.0511 0.0511 0.0511 0.0511\n",
      "177 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "178 165 165 165 165\n",
      "179 0.4000 0.4000 0.4000 0.4000\n",
      "180 P_100 P_100 P_100 P_100\n",
      "181 167 167 167 167\n",
      "182 0.1400 0.1400 0.1400 0.1400\n",
      "183 recall_100 recall_100 recall_100 recall_100\n",
      "184 167 167 167 167\n",
      "185 0.2333 0.2333 0.2333 0.2333\n",
      "186 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "187 167 167 167 167\n",
      "188 0.2006 0.2006 0.2006 0.2006\n",
      "189 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "190 167 167 167 167\n",
      "191 0.0434 0.0434 0.0434 0.0434\n",
      "192 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "193 167 167 167 167\n",
      "194 0.2333 0.2333 0.2333 0.2333\n",
      "195 P_100 P_100 P_100 P_100\n",
      "196 170 170 170 170\n",
      "197 0.0300 0.0300 0.0300 0.0300\n",
      "198 recall_100 recall_100 recall_100 recall_100\n",
      "199 170 170 170 170\n",
      "200 1.0000 1.0000 1.0000 1.0000\n",
      "201 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "202 170 170 170 170\n",
      "203 0.2616 0.2616 0.2616 0.2616\n",
      "204 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "205 170 170 170 170\n",
      "206 0.0418 0.0418 0.0418 0.0418\n",
      "207 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "208 170 170 170 170\n",
      "209 1.0000 1.0000 1.0000 1.0000\n",
      "210 P_100 P_100 P_100 P_100\n",
      "211 173 173 173 173\n",
      "212 0.7400 0.7400 0.7400 0.7400\n",
      "213 recall_100 recall_100 recall_100 recall_100\n",
      "214 173 173 173 173\n",
      "215 0.3645 0.3645 0.3645 0.3645\n",
      "216 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "217 173 173 173 173\n",
      "218 0.7465 0.7465 0.7465 0.7465\n",
      "219 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "220 173 173 173 173\n",
      "221 0.2698 0.2698 0.2698 0.2698\n",
      "222 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "223 173 173 173 173\n",
      "224 0.7400 0.7400 0.7400 0.7400\n",
      "225 P_100 P_100 P_100 P_100\n",
      "226 180 180 180 180\n",
      "227 0.2600 0.2600 0.2600 0.2600\n",
      "228 recall_100 recall_100 recall_100 recall_100\n",
      "229 180 180 180 180\n",
      "230 0.2016 0.2016 0.2016 0.2016\n",
      "231 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "232 180 180 180 180\n",
      "233 0.2945 0.2945 0.2945 0.2945\n",
      "234 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "235 180 180 180 180\n",
      "236 0.0675 0.0675 0.0675 0.0675\n",
      "237 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "238 180 180 180 180\n",
      "239 0.2600 0.2600 0.2600 0.2600\n",
      "240 P_100 P_100 P_100 P_100\n",
      "241 182 182 182 182\n",
      "242 0.2000 0.2000 0.2000 0.2000\n",
      "243 recall_100 recall_100 recall_100 recall_100\n",
      "244 182 182 182 182\n",
      "245 0.3846 0.3846 0.3846 0.3846\n",
      "246 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "247 182 182 182 182\n",
      "248 0.2848 0.2848 0.2848 0.2848\n",
      "249 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "250 182 182 182 182\n",
      "251 0.0758 0.0758 0.0758 0.0758\n",
      "252 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "253 182 182 182 182\n",
      "254 0.3846 0.3846 0.3846 0.3846\n",
      "255 P_100 P_100 P_100 P_100\n",
      "256 192 192 192 192\n",
      "257 0.6000 0.6000 0.6000 0.6000\n",
      "258 recall_100 recall_100 recall_100 recall_100\n",
      "259 192 192 192 192\n",
      "260 0.3352 0.3352 0.3352 0.3352\n",
      "261 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "262 192 192 192 192\n",
      "263 0.5562 0.5562 0.5562 0.5562\n",
      "264 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "265 192 192 192 192\n",
      "266 0.1818 0.1818 0.1818 0.1818\n",
      "267 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "268 192 192 192 192\n",
      "269 0.6000 0.6000 0.6000 0.6000\n",
      "270 P_100 P_100 P_100 P_100\n",
      "271 53 53 53 53\n",
      "272 0.4400 0.4400 0.4400 0.4400\n",
      "273 recall_100 recall_100 recall_100 recall_100\n",
      "274 53 53 53 53\n",
      "275 0.1630 0.1630 0.1630 0.1630\n",
      "276 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "277 53 53 53 53\n",
      "278 0.4311 0.4311 0.4311 0.4311\n",
      "279 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "280 53 53 53 53\n",
      "281 0.0776 0.0776 0.0776 0.0776\n",
      "282 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "283 53 53 53 53\n",
      "284 0.4400 0.4400 0.4400 0.4400\n",
      "285 P_100 P_100 P_100 P_100\n",
      "286 57 57 57 57\n",
      "287 0.3200 0.3200 0.3200 0.3200\n",
      "288 recall_100 recall_100 recall_100 recall_100\n",
      "289 57 57 57 57\n",
      "290 0.7805 0.7805 0.7805 0.7805\n",
      "291 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "292 57 57 57 57\n",
      "293 0.5910 0.5910 0.5910 0.5910\n",
      "294 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "295 57 57 57 57\n",
      "296 0.2634 0.2634 0.2634 0.2634\n",
      "297 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "298 57 57 57 57\n",
      "299 0.7805 0.7805 0.7805 0.7805\n",
      "300 P_100 P_100 P_100 P_100\n",
      "301 69 69 69 69\n",
      "302 0.0600 0.0600 0.0600 0.0600\n",
      "303 recall_100 recall_100 recall_100 recall_100\n",
      "304 69 69 69 69\n",
      "305 0.3750 0.3750 0.3750 0.3750\n",
      "306 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "307 69 69 69 69\n",
      "308 0.1794 0.1794 0.1794 0.1794\n",
      "309 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "310 69 69 69 69\n",
      "311 0.0236 0.0236 0.0236 0.0236\n",
      "312 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "313 69 69 69 69\n",
      "314 0.3750 0.3750 0.3750 0.3750\n",
      "315 P_100 P_100 P_100 P_100\n",
      "316 74 74 74 74\n",
      "317 0.0100 0.0100 0.0100 0.0100\n",
      "318 recall_100 recall_100 recall_100 recall_100\n",
      "319 74 74 74 74\n",
      "320 0.0054 0.0054 0.0054 0.0054\n",
      "321 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "322 74 74 74 74\n",
      "323 0.0129 0.0129 0.0129 0.0129\n",
      "324 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "325 74 74 74 74\n",
      "326 0.0005 0.0005 0.0005 0.0005\n",
      "327 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "328 74 74 74 74\n",
      "329 0.0100 0.0100 0.0100 0.0100\n",
      "330 P_100 P_100 P_100 P_100\n",
      "331 78 78 78 78\n",
      "332 0.7800 0.7800 0.7800 0.7800\n",
      "333 recall_100 recall_100 recall_100 recall_100\n",
      "334 78 78 78 78\n",
      "335 0.6047 0.6047 0.6047 0.6047\n",
      "336 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "337 78 78 78 78\n",
      "338 0.8044 0.8044 0.8044 0.8044\n",
      "339 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "340 78 78 78 78\n",
      "341 0.4979 0.4979 0.4979 0.4979\n",
      "342 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "343 78 78 78 78\n",
      "344 0.7800 0.7800 0.7800 0.7800\n",
      "345 P_100 P_100 P_100 P_100\n",
      "346 86 86 86 86\n",
      "347 0.1300 0.1300 0.1300 0.1300\n",
      "348 recall_100 recall_100 recall_100 recall_100\n",
      "349 86 86 86 86\n",
      "350 0.7222 0.7222 0.7222 0.7222\n",
      "351 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "352 86 86 86 86\n",
      "353 0.4221 0.4221 0.4221 0.4221\n",
      "354 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "355 86 86 86 86\n",
      "356 0.1163 0.1163 0.1163 0.1163\n",
      "357 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "358 86 86 86 86\n",
      "359 0.7222 0.7222 0.7222 0.7222\n",
      "360 P_100 P_100 P_100 P_100\n",
      "361 89 89 89 89\n",
      "362 0.0500 0.0500 0.0500 0.0500\n",
      "363 recall_100 recall_100 recall_100 recall_100\n",
      "364 89 89 89 89\n",
      "365 0.0877 0.0877 0.0877 0.0877\n",
      "366 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "367 89 89 89 89\n",
      "368 0.0603 0.0603 0.0603 0.0603\n",
      "369 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "370 89 89 89 89\n",
      "371 0.0043 0.0043 0.0043 0.0043\n",
      "372 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "373 89 89 89 89\n",
      "374 0.0877 0.0877 0.0877 0.0877\n",
      "375 P_100 P_100 P_100 P_100\n",
      "376 90 90 90 90\n",
      "377 0.2000 0.2000 0.2000 0.2000\n",
      "378 recall_100 recall_100 recall_100 recall_100\n",
      "379 90 90 90 90\n",
      "380 0.7143 0.7143 0.7143 0.7143\n",
      "381 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "382 90 90 90 90\n",
      "383 0.4213 0.4213 0.4213 0.4213\n",
      "384 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "385 90 90 90 90\n",
      "386 0.1286 0.1286 0.1286 0.1286\n",
      "387 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "388 90 90 90 90\n",
      "389 0.7143 0.7143 0.7143 0.7143\n",
      "390 P_100 P_100 P_100 P_100\n",
      "391 92 92 92 92\n",
      "392 0.0400 0.0400 0.0400 0.0400\n",
      "393 recall_100 recall_100 recall_100 recall_100\n",
      "394 92 92 92 92\n",
      "395 0.0870 0.0870 0.0870 0.0870\n",
      "396 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "397 92 92 92 92\n",
      "398 0.0558 0.0558 0.0558 0.0558\n",
      "399 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "400 92 92 92 92\n",
      "401 0.0032 0.0032 0.0032 0.0032\n",
      "402 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "403 92 92 92 92\n",
      "404 0.0870 0.0870 0.0870 0.0870\n",
      "405 P_100 P_100 P_100 P_100\n",
      "406 93 93 93 93\n",
      "407 0.4600 0.4600 0.4600 0.4600\n",
      "408 recall_100 recall_100 recall_100 recall_100\n",
      "409 93 93 93 93\n",
      "410 0.3087 0.3087 0.3087 0.3087\n",
      "411 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "412 93 93 93 93\n",
      "413 0.4371 0.4371 0.4371 0.4371\n",
      "414 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "415 93 93 93 93\n",
      "416 0.1446 0.1446 0.1446 0.1446\n",
      "417 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "418 93 93 93 93\n",
      "419 0.4600 0.4600 0.4600 0.4600\n",
      "420 P_100 P_100 P_100 P_100\n",
      "421 94 94 94 94\n",
      "422 0.1200 0.1200 0.1200 0.1200\n",
      "423 recall_100 recall_100 recall_100 recall_100\n",
      "424 94 94 94 94\n",
      "425 0.1714 0.1714 0.1714 0.1714\n",
      "426 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "427 94 94 94 94\n",
      "428 0.1549 0.1549 0.1549 0.1549\n",
      "429 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "430 94 94 94 94\n",
      "431 0.0253 0.0253 0.0253 0.0253\n",
      "432 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "433 94 94 94 94\n",
      "434 0.1714 0.1714 0.1714 0.1714\n",
      "435 P_100 P_100 P_100 P_100\n",
      "436 95 95 95 95\n",
      "437 0.0100 0.0100 0.0100 0.0100\n",
      "438 recall_100 recall_100 recall_100 recall_100\n",
      "439 95 95 95 95\n",
      "440 0.0303 0.0303 0.0303 0.0303\n",
      "441 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "442 95 95 95 95\n",
      "443 0.0218 0.0218 0.0218 0.0218\n",
      "444 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "445 95 95 95 95\n",
      "446 0.0012 0.0012 0.0012 0.0012\n",
      "447 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "448 95 95 95 95\n",
      "449 0.0303 0.0303 0.0303 0.0303\n",
      "450 P_100 P_100 P_100 P_100\n",
      "451 all all all all\n",
      "452 0.2313 0.2313 0.2313 0.2313\n",
      "453 recall_100 recall_100 recall_100 recall_100\n",
      "454 all all all all\n",
      "455 0.3207 0.3207 0.3207 0.3207\n",
      "456 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100 ndcg_cut_100\n",
      "457 all all all all\n",
      "458 0.2932 0.2932 0.2932 0.2932\n",
      "459 map_cut_100 map_cut_100 map_cut_100 map_cut_100\n",
      "460 all all all all\n",
      "461 0.1019 0.1019 0.1019 0.1019\n",
      "462 relative_P_100 relative_P_100 relative_P_100 relative_P_100\n",
      "463 all all all all\n",
      "464 0.3713 0.3713 0.3713 0.3713\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "a1=($(./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_circle_kernel:sigma=50:mu=100.run' | grep -E \"_100 \"))\n",
    "a2=($(./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_circle_kernel:sigma=50:mu=500.run' | grep -E \"_100 \"))\n",
    "a3=($(./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_circle_kernel:sigma=50:mu=1000.run' | grep -E \"_100 \"))\n",
    "a4=($(./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_validation' './plm_circle_kernel:sigma=50:mu=1500.run' | grep -E \"_100 \"))\n",
    "for index in \"${!a1[@]}\"\n",
    "do\n",
    "    echo \"$index ${a1[index]} ${a2[index]} ${a3[index]} ${a4[index]}\"\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "#Getting evaluation measures\n",
    "def parse_out(out):\n",
    "    return None\n",
    "def eval(run_doc):\n",
    "    # run_doc should be a filename\n",
    "    qrel_doc = \"/ap_88_89/qrel_test\"\n",
    "    qrel_doc = \"example.qrel\"\n",
    "    grep_string = \"^ndcg_cut_10\\s|^map_cut_1000\\s|^P_5\\s|^recall_1000\\s\"\n",
    "    eval_command = [\"./trec_eval/trec_eval\",\"-m\",\"all_trec\",\"-q\",qrel_doc,run_doc]\n",
    "    grep_command = [\"grep\",\"-E\",grep_string]\n",
    "    p = subprocess.Popen(eval_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    p_grep = subprocess.Popen(grep_command,stdin=p.stdout,stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out,err = p_grep.communicate()\n",
    "    print(out.decode(\"utf-8\"))\n",
    "    print(err.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_5                   \tQ1\t0.2000\n",
      "recall_1000           \tQ1\t1.0000\n",
      "ndcg_cut_10           \tQ1\t1.0000\n",
      "map_cut_1000          \tQ1\t1.0000\n",
      "P_5                   \tQ2\t0.2000\n",
      "recall_1000           \tQ2\t1.0000\n",
      "ndcg_cut_10           \tQ2\t0.6309\n",
      "map_cut_1000          \tQ2\t0.5000\n",
      "P_5                   \tall\t0.2000\n",
      "recall_1000           \tall\t1.0000\n",
      "ndcg_cut_10           \tall\t0.8155\n",
      "map_cut_1000          \tall\t0.7500\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval(\"example.run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Latent Semantic Models (LSMs) [15 points] ###\n",
    "\n",
    "In this task you will experiment with applying distributional semantics methods ([LSI](http://lsa3.colorado.edu/papers/JASIS.lsi.90.pdf) **[5 points]** and [LDA](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf) **[5 points]**) for retrieval.\n",
    "\n",
    "You do not need to implement LSI or LDA on your own. Instead, you can use [gensim](http://radimrehurek.com/gensim/index.html). An example on how to integrate Pyndri with Gensim for word2vec can be found [here](https://github.com/cvangysel/pyndri/blob/master/examples/word2vec.py). For the remaining latent vector space models, you will need to implement connector classes (such as `IndriSentences`) by yourself.\n",
    "\n",
    "In order to use a latent semantic model for retrieval, you need to:\n",
    "   * build a representation of the query **q**,\n",
    "   * build a representation of the document **d**,\n",
    "   * calculate the similarity between **q** and **d** (e.g., cosine similarity, KL-divergence).\n",
    "     \n",
    "The exact implementation here depends on the latent semantic model you are using. \n",
    "   \n",
    "Each of these LSMs come with various hyperparameters to tune. Make a choice on the parameters, and explicitly mention the reasons that led you to these decisions. You can use the validation set to optimize hyper parameters you see fit; motivate your decisions. In addition, mention clearly how the query/document representations were constructed for each LSM and explain your choices.\n",
    "\n",
    "In this experiment, you will first obtain an initial top-1000 ranking for each query using TF-IDF in **Task 1**, and then re-rank the documents using the LSMs. Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "Perform significance testing **[5 points]** (similar as in Task 1) in the class of semantic matching methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:  Word embeddings for ranking [20 points] (open-ended) ###\n",
    "\n",
    "First create word embeddings on the corpus we provided using [word2vec](http://arxiv.org/abs/1411.2738) -- [gensim implementation](https://radimrehurek.com/gensim/models/word2vec.html). You should extract the indexed documents using pyndri and provide them to gensim for training a model (see example [here](https://github.com/nickvosk/pyndri/blob/master/examples/word2vec.py)).\n",
    "   \n",
    "This is an open-ended task. It is left up you to decide how you will combine word embeddings to derive query and document representations. Note that since we provide the implementation for training word2vec, you will be graded based on your creativity on combining word embeddings for building query and document representations.\n",
    "\n",
    "Note: If you want to experiment with pre-trained word embeddings on a different corpus, you can use the word embeddings we provide alongside the assignment (./data/reduced_vectors_google.txt.tar.gz). These are the [google word2vec word embeddings](https://code.google.com/archive/p/word2vec/), reduced to only the words that appear in the document collection we use in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Word2vec init and training + model saving\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import copy\n",
    "import gensim\n",
    "import pyndri\n",
    "import pyndri.compat\n",
    "import sys\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "def modified_init_sims(self, replace=False):\n",
    "        \"\"\"\n",
    "        init_sims() resides in KeyedVectors because it deals with syn0 mainly, but because syn1 is not an attribute\n",
    "        of KeyedVectors, it has to be deleted in this class, and the normalizing of syn0 happens inside of KeyedVectors\n",
    "        \"\"\"\n",
    "        if replace and hasattr(self, 'syn1'):\n",
    "            pass\n",
    "            #del self.syn1 <- don't delete the matrice we need it \n",
    "        return self.wv.init_sims(replace)\n",
    "    \n",
    "def modified_minimize_model(self, save_syn1=False, save_syn1neg=False, save_syn0_lockf=False):\n",
    "    if save_syn1 and save_syn1neg and save_syn0_lockf:\n",
    "        return\n",
    "    if hasattr(self, 'syn1') and not save_syn1:\n",
    "        pass#del self.syn1\n",
    "    if hasattr(self, 'syn1neg') and not save_syn1neg:\n",
    "        pass#del self.syn1neg\n",
    "    if hasattr(self, 'syn0_lockf') and not save_syn0_lockf:\n",
    "        pass#del self.syn0_lockf\n",
    "    self.model_trimmed_post_training = True\n",
    "\n",
    "#replace init_sims with our sims method so that the W_out matrice does not get deleted\n",
    "Word2Vec.init_sims = modified_init_sims\n",
    "Word2Vec._minimize_model = modified_minimize_model\n",
    "\n",
    "word2vec_init = Word2Vec(\n",
    "    size=300,  # Embedding size\n",
    "    window=5,  # One-sided window size\n",
    "    sg=False,  # Skip-gram.\n",
    "    min_count=5,  # Minimum word frequency.\n",
    "    sample=1e-3,  # Sub-sample threshold.\n",
    "    hs=True,  # Hierarchical softmax.\n",
    "    negative=10,  # Number of negative examples.\n",
    "    iter=1,  # Number of iterations.\n",
    "    workers=8,  # Number of workers.\n",
    ")\n",
    "\n",
    "dictionary = pyndri.extract_dictionary(index)\n",
    "sentences = pyndri.compat.IndriSentences(index, dictionary)\n",
    "\n",
    "# Build vocab.\n",
    "word2vec_init.build_vocab(sentences, trim_rule=None)\n",
    "\n",
    "ITERS = 5\n",
    "\n",
    "models = [word2vec_init]\n",
    "print('Started training')\n",
    "for epoch in range(1, 5 + 1):\n",
    "    model = copy.deepcopy(models[-1])\n",
    "    model.train(sentences, total_examples = model.corpus_count, epochs = ITERS)\n",
    "\n",
    "    models.append(model)\n",
    "    print('Finished epoch ' + str(epoch))\n",
    "\n",
    "model_folder = 'models/task3/'\n",
    "model_name = 'init-word-embbedings'\n",
    "\n",
    "for index,model in enumerate(models):\n",
    "    model.save(model_folder + model_name + '_v' + str(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "model_folder = 'models/task3/'\n",
    "model_name = 'init-word-embbedings'\n",
    "use_pretrained = False\n",
    "version = 5\n",
    "\n",
    "if use_pretrained:\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format('reduced_vectors_google.txt')\n",
    "else:\n",
    "    model = gensim.models.Word2Vec.load(model_folder + model_name + '_v' + str(version))\n",
    "    \n",
    "#pprint(vars(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting top n doc query pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tfifd_top_n_doc_query_pair(n):\n",
    "    dic = {}\n",
    "    \n",
    "    counter = 0\n",
    "    previous_query_id = -1\n",
    "    with open('tfidf.run','r') as fd:\n",
    "        for line in fd:\n",
    "            q_id, _, ext_doc_id = line.strip().split(' ')[0:3]\n",
    "            int_doc_id = index.document_ids([ext_doc_id])[0][1]\n",
    "            \n",
    "            if previous_query_id != q_id:\n",
    "                #if new query\n",
    "                previous_query_id = q_id\n",
    "                counter = 0\n",
    "\n",
    "            if counter < n:\n",
    "                #add it if it's in top n\n",
    "                if int_doc_id not in dic.keys():\n",
    "                     dic[int_doc_id] = []\n",
    "                dic[int_doc_id].append(q_id)\n",
    "            else:\n",
    "                #skip if not in top n\n",
    "                pass\n",
    "            counter += 1\n",
    "            \n",
    "    return dic\n",
    "\n",
    "start_time = time.time()\n",
    "top_n = 100\n",
    "doc_query_top_n_dic = get_tfifd_top_n_doc_query_pair(top_n)\n",
    "print('creating top ' + str(top_n) + ' doc to query dict took:' + str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining retrieval and scoring functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2vec_retrieval(model_name, score_fn, out):\n",
    "    \"\"\"\n",
    "    mu = 0 means no dirichlet smoothing\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "    \n",
    "    :param model_name: the name of the model (a string)\n",
    "    :param score_fn: the scoring function (a function - see below for an example) \n",
    "    \"\"\"\n",
    "    run_out_path = '{}.run'.format(model_name)\n",
    "\n",
    "    if os.path.exists(run_out_path):\n",
    "        return\n",
    "\n",
    "    retrieval_start_time = time.time()\n",
    "\n",
    "    print('Retrieving using', model_name)\n",
    "    data = {}\n",
    "    \n",
    "    count_docs = 0\n",
    "    for int_doc_id in doc_query_top_n_dic.keys():\n",
    "        #for each document in the top 1000\n",
    "        \n",
    "        count_docs += 1\n",
    "            \n",
    "        if count_docs % 1000 == 0:\n",
    "            print(str(count_docs) + ' docs in: ' +str(time.time() - start_time))\n",
    "        query_docs_score_list = []    \n",
    "        for query_id in doc_query_top_n_dic[int_doc_id]:\n",
    "\n",
    "            query_token_list = tokenized_queries[query_id]\n",
    "            \n",
    "            #for each query of the document in the top 1000    \n",
    "            ext_doc_id, doc_terms_ids = index.document(int_doc_id)\n",
    "            \n",
    "            q_d_score = w2v_scoring_method_document_query(score_fn, int_doc_id, query_token_list, out)\n",
    "            #print('DESM score ' + str(q_d_score) + ' for query ' + str(query_id) + ' document id' + str(int_doc_id))\n",
    "            \n",
    "            query_docs_score_list.append((q_d_score, ext_doc_id))\n",
    "        \n",
    "        data[query_id] = query_docs_score_list\n",
    "        \n",
    "    with open(run_out_path, 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=data,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)\n",
    "        \n",
    "def w2v_scoring_method_document_query(score_fn, int_doc_id, query_token_list, out):\n",
    "    score = 0\n",
    "    \n",
    "    for token_id in query_token_list:\n",
    "        params={'int_document_id':int_doc_id,'query_term_id':token_id,'question_len':len(query_token_list), 'out':out}\n",
    "        score += score_fn(**params)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DESM approach based on://arxiv.org/pdf/1602.01137.pdf\n",
    "\n",
    "import math\n",
    "import numpy\n",
    "\n",
    "'''\n",
    "Calculates the centroid of all the normalized vectors for the words \n",
    "in the document serving as a single embedding for the whole document\n",
    "'''\n",
    "def calculate_centroid(int_document_id, out = True):\n",
    "    _, doc_terms_ids = index.document(int_document_id)\n",
    "    \n",
    "    terms = [id2token[word_id] for word_id in doc_terms_ids if word_id > 0]\n",
    "    doc_length = len(doc_terms_ids)\n",
    "    sum_of_normalized_vectors = 0\n",
    "    skipped_terms = 0\n",
    "    \n",
    "    if doc_length == 0:\n",
    "        print('Doc length 0, why? doc_id: ' + str(int_document_id))\n",
    "        return False\n",
    "\n",
    "    if not out:\n",
    "        for term in terms:\n",
    "            if term in model.wv:\n",
    "                word2vec_repr = model.wv[term]\n",
    "                l2_norm = math.sqrt(sum([i*i for i in word2vec_repr]))\n",
    "                sum_of_normalized_vectors += word2vec_repr / l2_norm\n",
    "            else:\n",
    "                skipped_terms += 1\n",
    "    else:\n",
    "        for term in terms:\n",
    "            if term in model.wv:\n",
    "                term_index = model.wv.vocab[term].index\n",
    "                word2vec_repr = model.syn1[term_index]\n",
    "                l2_norm = math.sqrt(sum([i*i for i in word2vec_repr]))\n",
    "                sum_of_normalized_vectors += word2vec_repr / l2_norm\n",
    "            else:\n",
    "                skipped_terms += 1\n",
    "            \n",
    "    if skipped_terms == len(terms):\n",
    "        print('All terms skipped, why? doc_id: ' + str(int_document_id))\n",
    "        return False\n",
    "    \n",
    "    return sum_of_normalized_vectors / doc_length\n",
    "\n",
    "\n",
    "'''\n",
    "Creates a matrix storing 'doc_id' => 'embedding_represention_of_doc'\n",
    "'''\n",
    "def calculate_documents_centroid_matrix(out = True):\n",
    "    document_centroid_matrix = dict()\n",
    "    for int_doc_id in range(index.document_base(), 5):\n",
    "        centroid_value = calculate_centroid(int_doc_id, out)\n",
    "        document_centroid_matrix[str(int_doc_id)] = centroid_value\n",
    "        \n",
    "        if int_doc_id % 5000 == 0:\n",
    "            print('Finished 10000 documents')\n",
    "\n",
    "    return document_centroid_matrix\n",
    "\n",
    "'''\n",
    "Computes the corresponding DESM given a query term and a document \n",
    "'''\n",
    "def DESM(int_document_id, query_term_id, question_len, out):\n",
    "    D = calculate_centroid(int_document_id, out)\n",
    "    \n",
    "    if D is False:\n",
    "        return 0\n",
    "\n",
    "    token = id2token[query_term_id]\n",
    "    \n",
    "    if token not in model.wv:\n",
    "        return 0\n",
    "    else:\n",
    "        word2vec_repr = model.wv[token]\n",
    "    \n",
    "    l2_norm_d = math.sqrt(sum([i*i for i in D]))\n",
    "    l2_norm_q = math.sqrt(sum([i*i for i in word2vec_repr]))\n",
    "    denominator = l2_norm_d * l2_norm_q * question_len\n",
    "     \n",
    "    nominator = np.dot(np.array(word2vec_repr), np.array(D))\n",
    "    DESM_score = nominator / denominator\n",
    "    \n",
    "    return DESM_score\n",
    "\n",
    "'''\n",
    "Computes the corresping DESM score for a query term and a document\n",
    "'''\n",
    "def DESM_full_query(int_document_id, query_terms_id, out):\n",
    "    question_len = len(query_terms_id)\n",
    "    score = 0.0\n",
    "    \n",
    "    for query_term_id in query_terms_id:\n",
    "        score += DESM(int_document_id, query_term_id, question_len, out)\n",
    "    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "word2vec_retrieval('desm', DESM, out=False)  \n",
    "print('DESM on query-docs took: ' + str(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_test' './desm.run' | grep -E \"^*all\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "word2vec_retrieval('desmINOUT', DESM, out=True)  \n",
    "print('DESM on query-docs took: ' + str(time.time()-start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "./trec_eval/trec_eval -m all_trec -q './ap_88_89/qrel_test' './desmINOUT.run' | grep -E \"^*all\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Learning to rank (LTR) [15 points] (open-ended) ###\n",
    "\n",
    "In this task you will get an introduction into learning to rank for information retrieval.\n",
    "\n",
    "You can explore different ways for devising features for the model. Obviously, you can use the retrieval methods you implemented in Task 1, Task 2 and Task 3 as features. Think about other features you can use (e.g. query/document length). Creativity on devising new features and providing motivation for them will be taken into account when grading.\n",
    "\n",
    "For every query, first create a document candidate set using the top-1000 documents using TF-IDF, and subsequently compute features given a query and a document. Note that the feature values of different retrieval methods are likely to be distributed differently.\n",
    "\n",
    "You are adviced to start some pointwise learning to rank algorithm e.g. logistic regression, implemented in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "Train your LTR model using 10-fold cross validation on the test set. More advanced learning to rank algorithms will be appreciated when grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 4: Write a report [15 points; instant FAIL if not provided] ###\n",
    "\n",
    "The report should be a PDF file created using the [sigconf ACM template](https://www.acm.org/publications/proceedings-template) and will determine a significant part of your grade.\n",
    "\n",
    "   * It should explain what you have implemented, motivate your experiments and detail what you expect to learn from them. **[10 points]**\n",
    "   * Lastly, provide a convincing analysis of your results and conclude the report accordingly. **[10 points]**\n",
    "      * Do all methods perform similarly on all queries? Why?\n",
    "      * Is there a single retrieval model that outperforms all other retrieval models (i.e., silver bullet)?\n",
    "      * ...\n",
    "\n",
    "**Hand in the report and your self-contained implementation source files.** Only send us the files that matter, organized in a well-documented zip/tgz file with clear instructions on how to reproduce your results. That is, we want to be able to regenerate all your results with minimal effort. You can assume that the index and ground-truth information is present in the same file structure as the one we have provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of pairwise ranking using scikit-learn LinearSVC\n",
    "Reference: \n",
    "    \"Large Margin Rank Boundaries for Ordinal Regression\", R. Herbrich,\n",
    "    T. Graepel, K. Obermayer 1999\n",
    "    \"Learning to rank from medical imaging data.\" Pedregosa, Fabian, et al., \n",
    "    Machine Learning in Medical Imaging 2012.\n",
    "Authors: Fabian Pedregosa <fabian@fseoane.net>\n",
    "         Alexandre Gramfort <alexandre.gramfort@inria.fr>\n",
    "See also https://github.com/fabianp/pysofia for a more efficient implementation\n",
    "of RankSVM using stochastic gradient descent methdos.\n",
    "\"\"\"\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm, linear_model, cross_validation\n",
    "\n",
    "\n",
    "def transform_pairwise(X, y):\n",
    "    \"\"\"Transforms data into pairs with balanced labels for ranking\n",
    "    Transforms a n-class ranking problem into a two-class classification\n",
    "    problem. Subclasses implementing particular strategies for choosing\n",
    "    pairs should override this method.\n",
    "    In this method, all pairs are choosen, except for those that have the\n",
    "    same target value. The output is an array of balanced classes, i.e.\n",
    "    there are the same number of -1 as +1\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n_samples, n_features)\n",
    "        The data\n",
    "    y : array, shape (n_samples,) or (n_samples, 2)\n",
    "        Target labels. If it's a 2D array, the second column represents\n",
    "        the grouping of samples, i.e., samples with different groups will\n",
    "        not be considered.\n",
    "    Returns\n",
    "    -------\n",
    "    X_trans : array, shape (k, n_feaures)\n",
    "        Data as pairs\n",
    "    y_trans : array, shape (k,)\n",
    "        Output class labels, where classes have values {-1, +1}\n",
    "    \"\"\"\n",
    "    X_new = []\n",
    "    y_new = []\n",
    "    y = np.asarray(y)\n",
    "    if y.ndim == 1:\n",
    "        y = np.c_[y, np.ones(y.shape[0])]\n",
    "    comb = itertools.combinations(range(X.shape[0]), 2)\n",
    "    for k, (i, j) in enumerate(comb):\n",
    "        if y[i, 0] == y[j, 0] or y[i, 1] != y[j, 1]:\n",
    "            # skip if same target or different group\n",
    "            continue\n",
    "        X_new.append(X[i] - X[j])\n",
    "        y_new.append(np.sign(y[i, 0] - y[j, 0]))\n",
    "        # output balanced classes\n",
    "        if y_new[-1] != (-1) ** k:\n",
    "            y_new[-1] = - y_new[-1]\n",
    "            X_new[-1] = - X_new[-1]\n",
    "    return np.asarray(X_new), np.asarray(y_new).ravel()\n",
    "\n",
    "\n",
    "class RankSVM(svm.LinearSVC):\n",
    "    \"\"\"Performs pairwise ranking with an underlying LinearSVC model\n",
    "    Input should be a n-class ranking problem, this object will convert it\n",
    "    into a two-class classification problem, a setting known as\n",
    "    `pairwise ranking`.\n",
    "    See object :ref:`svm.LinearSVC` for a full description of parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose):\n",
    "        \n",
    "        super(RankSVM, self).__init__(verbose=verbose, max_iter=1)\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit a pairwise ranking model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "        y : array, shape (n_samples,) or (n_samples, 2)\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        print('here', self.verbose)\n",
    "        X_trans, y_trans = transform_pairwise(X, y)\n",
    "        super(RankSVM, self).fit(X_trans, y_trans)\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        return np.dot(X, self.coef_.ravel())\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict an ordering on X. For a list of n samples, this method\n",
    "        returns a list from 0 to n-1 with the relative order of the rows of X.\n",
    "        The item is given such that items ranked on top have are\n",
    "        predicted a higher ordering (i.e. 0 means is the last item\n",
    "        and n_samples would be the item ranked on top).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "        Returns\n",
    "        -------\n",
    "        ord : array, shape (n_samples,)\n",
    "            Returns a list of integers representing the relative order of\n",
    "            the rows in X.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'coef_'):\n",
    "            return np.argsort(np.dot(X, self.coef_.ravel()))\n",
    "        else:\n",
    "            raise ValueError(\"Must call fit() prior to predict()\")\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Because we transformed into a pairwise problem, chance level is at 0.5\n",
    "        \"\"\"\n",
    "        X_trans, y_trans = transform_pairwise(X, y)\n",
    "        return np.mean(super(RankSVM, self).predict(X_trans) == y_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(int_doc_id, query):\n",
    "    #also maybe include lsi, lda, dsea\n",
    "    \n",
    "    features = []\n",
    "    #log(doc_len)\n",
    "    features.append(np.log(document_lengths[int_doc_id]))\n",
    "    \n",
    "    #f_coverage_score = coverage_score(int_doc_id, query)\n",
    "    \n",
    "    models_params = [(tfidf, {}), (bm25, {'k1':1.2, 'b':0.75}), (jelinek_mercer_smoothing, {'lambda_jm':0.5}), \n",
    "                    (absolute_discounting, {'delta':0.5}), (dirichlet_prior_smoothing, {'mu':100})]\n",
    "    \n",
    "    for model_param in models_params:\n",
    "        features.append(multinomial_scoring_method_document_query(model_param[0], model_param[1], int_doc_id, query))\n",
    "    \n",
    "    return np.asarray(features)\n",
    "\n",
    "    \n",
    "def create_dataset(qrel_file):\n",
    "    with open('ap_88_89/topics_title', 'r') as fd:\n",
    "        queries = parse_topics([fd])\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    #68 0 AP880218-0195 1\n",
    "    with open(qrel_file, 'r') as fd:\n",
    "        for line in fd:\n",
    "            q_id,  _, ext_doc_id, true_score = line.split(\" \")\n",
    "            \n",
    "            if q_id in queries:\n",
    "                query_tokens = index.tokenize(queries[q_id])\n",
    "                query = [token2id.get(query_token,0) for query_token in query_tokens]\n",
    "                query = [word_id for word_id in query if word_id > 0]\n",
    "                \n",
    "                if not index.document_ids([ext_doc_id]):\n",
    "                    continue\n",
    "                    \n",
    "                int_doc_id = index.document_ids([ext_doc_id])[0][1]\n",
    "                doc = index.document(int_doc_id)\n",
    "\n",
    "                x.append(get_features(int_doc_id, query))\n",
    "                y.append(float(true_score[0]))\n",
    "        \n",
    "    return (np.asarray(x), np.asarray(y))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y = create_dataset('ap_88_89/qrel_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankSVM(verbose=1)\n",
      "here 1\n"
     ]
    }
   ],
   "source": [
    "def run_svm(X, y, verbose):\n",
    "    np.random.seed(1)\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    cv = cross_validation.KFold(n_samples, 10)\n",
    "    train, test = next(iter(cv))\n",
    "\n",
    "    # print the performance of ranking\n",
    "    rank_svm = RankSVM(verbose=verbose)\n",
    "    print(rank_svm)\n",
    "    rank_svm.fit(X[train], y[train])\n",
    "    print('Performance of ranking ', rank_svm.score(X[test], y[test]))\n",
    "\n",
    "    return rank_svm\n",
    "\n",
    "def run_lin_reg(X, y):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    cv = cross_validation.KFold(n_samples, 10)\n",
    "    train, test = next(iter(cv))\n",
    "    \n",
    "    # and that of linear regression\n",
    "    ridge = linear_model.RidgeCV(fit_intercept=True)\n",
    "    ridge.fit(X[train], y[train])\n",
    "    X_test_trans, y_test_trans = transform_pairwise(X[test], y[test])\n",
    "    score = np.mean(np.sign(np.dot(X_test_trans, ridge.coef_)) == y_test_trans)\n",
    "    print('Performance of linear regression ', score)\n",
    "    \n",
    "svm = run_svm(X,y,verbose=1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
